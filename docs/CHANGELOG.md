# Changelog

All notable changes to sai3-bench will be documented in this file.

## [0.7.0] - 2025-10-31

### üå≥ Directory Tree Workloads & Filesystem Testing

**Major feature release**: Comprehensive filesystem testing capabilities with directory tree workloads and nested path support for both traditional filesystems and object storage. Includes operations for directory management (MKDIR, RMDIR), path enumeration (LIST), metadata queries (STAT), and cleanup (DELETE) across all storage backends.

#### New Features

- **Directory Tree Workloads** - Hierarchical filesystem structure generation
  - Configurable tree dimensions: `width` (subdirs per level) √ó `depth` (tree levels)
  - Multiple distribution strategies:
    - `bottom`: Files only in leaf directories (traditional pattern)
    - `all`: Files at every level (root + intermediate + leaf)
  - Flexible size specifications: `uniform`, `lognormal`, `fixed`
  - Random fill default for realistic compression-resistant data
  - Full cloud storage compatibility (S3, Azure Blob, GCS)
  
- **Enhanced `--dry-run` Output** - Comprehensive pre-execution validation
  - Total directories count
  - Total files count
  - Total data size (bytes + human-readable: TiB/GiB/MiB/KiB/B)
  - Accurate calculations using TreeManifest and SizeGenerator
  - Example: `Total Directories: 12, Total Files: 60, Total Data: 600 KiB (614400 bytes)`

- **Filesystem Operations** - Full support for nested paths and directory operations
  - **Directory management**: MKDIR (create), RMDIR (remove) for filesystem backends
  - **Path enumeration**: LIST operations with prefix filtering (all backends)
  - **Metadata queries**: STAT operations for object metadata (all backends)
  - **Cleanup operations**: DELETE for files and objects (all backends)
  - **Nested path support**: Works with both real directories (file://) and key prefixes (s3://, az://, gs://)
  - Conditional mkdir: Automatically skips for object storage (implicit directories)
  - Cloud metadata trait with backend-specific implementations

- **PathSelector** - Dynamic file selection during workload execution
  - Weighted random selection across directory tree
  - GET/PUT/STAT operation support
  - Distributed coordination with TreeManifest
  - Collision-free file numbering across agents

- **Distributed Tree Coordination** - TreeManifest for multi-agent consistency
  - Shared directory structure across distributed agents
  - Per-agent file ranges prevent collisions
  - Serializable manifest for gRPC transmission
  - Accurate total tracking: directories, files, data size

#### Configuration Changes

- **Default fill type changed**: Zero ‚Üí Random
  - **Breaking Change**: Configs without explicit `fill:` now use random data
  - **Rationale**: Random fill provides more realistic testing (compression-resistant)
  - **Migration**: Add `fill: zero` to configs if zero fill required
  
- **New config section**: `directory_tree`
  ```yaml
  directory_tree:
    width: 3              # Subdirectories per level
    depth: 2              # Tree depth
    files_per_dir: 10     # Files per directory
    distribution: bottom  # "bottom" or "all"
    size:
      type: uniform
      min_size_kb: 4
      max_size_kb: 16
    fill: random          # "random" (default), "zero", "sequential"
  ```

#### Testing & Validation

- **101 total tests** (+40 from v0.6.11) - All passing
  - 38 lib tests (core functionality)
  - 30 directory tree creation tests (new)
  - 8 distributed config tests
  - 8 controller simulation tests
  - 6 streaming replay tests
  - 8 GCS backend tests
  - 2 utility tests
  - 1 gRPC test

- **Azure Blob Storage validation**:
  - Bottom distribution: 90 files in 9 leaf directories (~11.64 ops/s)
  - All-levels distribution: 60 files across 12 directories (~11.04 ops/s)
  - Performance: GET ~900ms, PUT ~450ms, STAT ~430ms
  - Verified with s3-cli ls command (correct hierarchical structure)

- **Comprehensive test configurations** (9 new configs in `tests/configs/directory-tree/`):
  - `tree_test_basic.yaml` - Simple 2√ó2 tree
  - `tree_test_bottom.yaml` - Bottom distribution (leaf only)
  - `tree_test_all_levels.yaml` - All-levels distribution
  - `tree_test_fixed_size.yaml` - Fixed size files
  - `tree_test_lognormal.yaml` - Lognormal distribution
  - `tree_test_io_operations.yaml` - Operation mix testing
  - `tree_test_azure_blob.yaml` - Azure Blob example (tested)
  - `tree_test_azure_all_levels.yaml` - Azure all-levels (tested)
  - `tree_test_s3_example.yaml` - S3 template

- **Metadata test configurations** (6 new configs):
  - `metadata_simple.yaml` - Basic operations
  - `metadata_file_test.yaml` - File-based testing
  - `metadata_stress_test.yaml` - Stress testing
  - `mkdir_put_race_test.yaml` - Race conditions
  - `mkdir_rmdir_race_test.yaml` - Create/delete races
  - `rmdir_nonempty_test.yaml` - Non-empty handling

#### Implementation Details

- **New modules**:
  - `src/directory_tree.rs` (824 lines) - Core tree generation, PathSelector, TreeManifest
  - `src/cloud_metadata.rs` (382 lines) - Unified metadata trait, backend implementations

- **Enhanced modules**:
  - `src/workload.rs` (+798 lines) - Metadata ops integration, tree workload execution
  - `src/config.rs` (+116 lines) - DirectoryTreeConfig, DataFillType enum
  - `src/main.rs` (+90 lines) - Enhanced dry-run with metrics

- **Backend compatibility**:
  | Backend | MKDIR | RMDIR | LIST | STAT | DELETE | Tree Workloads |
  |---------|-------|-------|------|------|--------|----------------|
  | file:// | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ Tested |
  | direct:// | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ Compatible |
  | s3:// | ‚ö†Ô∏è Skip | ‚ö†Ô∏è Skip | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ Compatible |
  | az:// | ‚ö†Ô∏è Skip | ‚ö†Ô∏è Skip | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ **TESTED** |
  | gs:// | ‚ö†Ô∏è Skip | ‚ö†Ô∏è Skip | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ Compatible |

  *‚ö†Ô∏è Skip = Implicit directories (no mkdir needed)*

#### Documentation

- **New documentation** (5,800+ lines):
  - `docs/PHASE1_METADATA_OPERATIONS.md` (598 lines) - Metadata design
  - `docs/PHASE_2B_FILE_INTEGRATION_PLAN.md` (684 lines) - PathSelector design
  - `docs/DIRECTORY_TREE_SHARED_FILESYSTEM_DESIGN.md` (488 lines) - Tree design
  - `docs/RDF_BENCH_ANALYSIS.md` (732 lines) - Collision avoidance analysis
  - `docs/BLOCK_IO_IMPLEMENTATION_PLAN.md` (855 lines) - Future direct I/O
  - `docs/v0.7.0-gap-analysis.md` (395 lines) - Gap analysis & roadmap
  - `tests/configs/directory-tree/README.md` (252 lines) - Testing guide

- **Updated `.github/copilot-instructions.md`** (+161 lines):
  - **Critical debugging rule**: Verbose flags (-v/-vv/-vvv) MUST come BEFORE subcommands
  - Never use RUST_LOG, use built-in verbosity flags
  - Zero warnings policy reinforcement

#### Bug Fixes

- **rdf-bench collision bug**: Fixed file numbering in distributed mode
  - Analysis documented in `docs/RDF_BENCH_ANALYSIS.md`
  - TreeManifest ensures collision-free numbering across agents
  - Per-agent file ranges with proper offset calculation

#### Performance

- **Azure Blob Storage** (tested with 60-90 file hierarchies):
  - Overall throughput: 11-12 ops/s
  - GET latency: mean ~900ms, p50 ~915ms, p95 ~1040ms
  - PUT latency: mean ~450ms, p50 ~430ms, p95 ~620ms
  - STAT latency: mean ~430ms, p50 ~430ms, p95 ~520ms

#### Breaking Changes

1. **Default fill type**: Zero ‚Üí Random
   - Impact: Configs without explicit `fill:` will use random data
   - Migration: Add `fill: zero` if zero fill required

2. **Object storage mkdir behavior**: Now skips mkdir for s3://, az://, gs://
   - Impact: Slight performance improvement for cloud storage
   - Benefit: Eliminates unnecessary API calls

#### Migration Guide

**For existing configs:**
1. Review fill type - add `fill: zero` if needed
2. No action needed for operation mix, sizes, concurrency

**For new directory tree workloads:**
1. Start with `tests/configs/directory-tree/tree_test_basic.yaml`
2. Test with `--dry-run` to validate structure
3. Review [Directory Tree README](../tests/configs/directory-tree/README.md)

#### Statistics

- **Total changes**: 9,884 insertions, 81 deletions across 49 files
- **18 commits** in feature branch
- **101 tests passing** (100% pass rate)
- **Zero compiler warnings**
- **3 backends validated**: file://, direct://, az://

---

## [0.6.11] - 2025-10-20

### ÔøΩ SSH-Automated Distributed Testing

**Major feature release**: Zero-touch distributed deployment with automated SSH, per-agent configuration, and flexible container runtime support (Docker/Podman).

#### New Features

- **SSH Automation** - One-command setup for passwordless access
  - `sai3bench-ctl ssh-setup --hosts vm1,vm2,vm3` - Automated SSH key generation, distribution, and verification
  - Automatic Docker/Podman availability checking
  - See [SSH Setup Guide](SSH_SETUP_GUIDE.md)

- **Config-Driven Agents** - Define all agents in YAML (no CLI flags needed)
  ```yaml
  distributed:
    agents:
      - address: "vm1.example.com"
        target_override: "s3://bucket-1/"
        concurrency_override: 128
        env: { RUST_LOG: "debug" }
  ```

- **Flexible Container Runtime** - Docker or Podman via YAML (no recompilation)
  ```yaml
  deployment:
    container_runtime: "docker"  # or "podman"
    image: "sai3bench:v0.6.11"
  ```

- **Per-Agent Customization**
  - Override target storage backend per agent
  - Custom concurrency levels per agent
  - Environment variable injection
  - Volume mounts for local storage testing

- **Automated Deployment** - Controller handles full lifecycle
  - SSH to VMs, pull images, start containers
  - Health checks and coordinated start
  - Graceful Ctrl+C handling with cleanup
  - Automatic container stop/removal on completion

- **Scale-Out & Scale-Up Support** - Flexible deployment strategies
  - **Scale-Out**: Multiple VMs, one container each (max network bandwidth)
  - **Scale-Up**: One large VM, multiple containers on different ports (cost optimization)
  - See [Scale-Out vs Scale-Up Guide](SCALE_OUT_VS_SCALE_UP.md)

#### Cloud Automation Scripts

- **GCP Script** (`scripts/gcp_distributed_test.sh`) - Production-ready GCP automation
  - Automated VM creation with Docker pre-installed
  - Workload config generation
  - Results collection and optional cleanup
  - 559 lines of complete lifecycle management

- **Cloud Templates** (`scripts/cloud_test_template.sh`) - Adapt for AWS/Azure
  - Generic template with example implementations
  - Cloud-agnostic interface functions
  
- **Local Testing** (`scripts/local_docker_test.sh`) - Test distributed mode locally
  - No cloud resources needed
  - Optional MinIO for S3 simulation
  - Perfect for CI/CD pipelines

#### Documentation

- **[Distributed Testing Guide](DISTRIBUTED_TESTING_GUIDE.md)** - Complete workflows, scale-out vs scale-up patterns, Podman support
- **[SSH Setup Guide](SSH_SETUP_GUIDE.md)** - One-command SSH automation, troubleshooting
- **[Scale-Out vs Scale-Up](SCALE_OUT_VS_SCALE_UP.md)** - Performance comparison, cost analysis, use cases
- **[Cloud Test Scripts](../scripts/README.md)** - GCP automation, AWS/Azure templates, local testing

#### Testing

- **60 total tests** (24 lib, 11 distributed config, 8 controller simulation, 8 GCS, 1 gRPC, 6 streaming, 2 utils)
- **58/60 passing** (97%) - Only 2 non-critical doc test failures
- **Config validation**: All distributed YAML schemas validated
- **Simulation tests**: Controller logic verified without real VMs

### ÔøΩüêõ Bug Fixes

- **Bucket label inconsistency**: Fixed inconsistent BUCKET_LABELS definition between `metrics.rs` (">2GiB") and `controller.rs` ("2GiB+")
  - Established single source of truth: `controller.rs` now imports from `metrics.rs`
  - Removed duplicate const definition
  - Ensures consistent histogram reporting across distributed and single-node modes

### ‚úÖ Test Improvements

- **Comprehensive bucket boundary testing**: Added missing test coverage for 512 KiB boundary
  - New `test_all_bucket_boundaries_comprehensive()` with 100+ assertions
  - Validates all 9 bucket boundaries, transitions, and edge cases
  - Confirms bucket index function produces correct results
  
- **Streaming replay test stability**: Fixed 2/6 test failures due to parallel execution
  - Added `serial_test = "3.2"` dependency
  - Applied `#[serial]` attribute to all 6 streaming replay tests
  - Resolved "incomplete frame" errors caused by global op-logger singleton conflicts

- **Test suite status**: 60 tests total, 58 passing (97%) - up from 41 tests

### üìù Files Modified

**Core Implementation**:
- `src/config.rs`: Added DistributedConfig, AgentConfig, SshConfig, DeploymentConfig (+180 lines)
- `src/ssh_deploy.rs`: SSH deployment automation (NEW, 360 lines)
- `src/ssh_setup.rs`: SSH key setup wizard (NEW, 300 lines)
- `src/bin/controller.rs`: Config-driven agent discovery, SSH automation, Ctrl+C handling (+120 lines)
- `Cargo.toml`: Added ssh2, shellexpand dependencies

**Example Configs**:
- `examples/distributed-ssh-automated.yaml`: Complete SSH automation example
- `examples/distributed-scale-up.yaml`: 8 containers on 1 VM
- `examples/distributed-scale-out.yaml`: 8 VMs, 1 container each

**Testing**:
- `tests/distributed_config_tests.rs`: 11 YAML parsing validation tests (NEW)
- `tests/distributed_simulation_test.rs`: 8 controller logic tests (NEW)
- `tests/configs/distributed_yaml_test.yaml`: Test configuration

**Cloud Automation**:
- `scripts/gcp_distributed_test.sh`: Complete GCP automation (NEW, 559 lines)
- `scripts/cloud_test_template.sh`: Cloud-agnostic template (NEW, 406 lines)
- `scripts/local_docker_test.sh`: Local multi-container testing (NEW, 399 lines)
- `scripts/validate_gcp_script.sh`: Config validation without VMs (NEW)
- `scripts/README.md`: Complete script documentation
- `scripts/QUICKSTART.md`: Progressive cloud testing guide

### üìö Documentation Consolidation

**Archived** (superseded by comprehensive distributed guides):
- `docs/V0.6.4_MULTIHOST_SUMMARY.md` - Functionality now in DISTRIBUTED_TESTING_GUIDE.md
- `docs/V0.6.4_TESTING_SUMMARY.md` - Results directory feature completed in v0.6.4

**Updated**:
- `README.md`: Feature-focused distributed section with links to guides

## [0.6.10] - 2025-10-19

### üî¨ s3dlio v0.9.10 Integration + Performance Analysis

**Upgraded to s3dlio v0.9.10** with new object size pre-fetching and optimized GET capabilities. Comprehensive GCS testing revealed important performance characteristics - pre-stat and RangeEngine optimizations provide NO benefit for same-region, high-bandwidth scenarios.

#### Key Findings

- **Pre-stat optimization**: Batch HEAD requests (~250ms for 1000 objects) do NOT improve performance for same-region cloud storage
  - Testing showed <1% difference (within measurement noise)
  - Root cause: Network bandwidth saturated (~2.6 GB/s), not HEAD-request bound
  - **Solution**: Gated behind `range_engine.enabled` flag to avoid 250ms overhead when not needed

- **RangeEngine overhead**: Parallel chunk downloads are 35% SLOWER than single-stream for same-region
  - Tested with 4MB and 8MB chunks across 4-32MB objects
  - CPU utilization: 100% baseline vs 80% RangeEngine (coordination overhead > parallelism benefit)
  - **Conclusion**: Use RangeEngine only for cross-region or low-bandwidth scenarios

#### New Features

- **Object size caching** via s3dlio v0.9.10
  - Thread-safe ObjectSizeCache with 60s TTL for GCS
  - Pre-stat support: `store.pre_stat_and_cache()` for batch HEAD requests
  - Available when `range_engine.enabled = true` in config

- **Optimized GET path** for cloud storage
  - Switched to `get_optimized()` for s3://, gs://, az:// URIs
  - Cache-aware with automatic RangeEngine integration for large objects
  - Preserves simple `get()` for local file:// and direct:// backends

#### Configuration

Pre-stat only runs when RangeEngine is enabled:
```yaml
range_engine:
  enabled: true      # Enables pre-stat + parallel chunk downloads
  chunk_size_mb: 4   # Minimum 4MB chunks
  max_workers: 8     # Parallel chunk workers
```

**Recommendation**: Keep RangeEngine disabled (default) for same-region workloads. Enable only for:
- Cross-region transfers with high latency
- Low-bandwidth network links
- Scenarios where parallelism helps more than coordination costs

#### Documentation

- **V0.6.10_PERFORMANCE_ANALYSIS.md**: Comprehensive GCS testing methodology, results, and recommendations
- **v0.6.9 benchmark configs**: Baseline performance data for 4-32MB objects
- **v0.6.10 test configs**: 16 GCS test scenarios (prepare + baseline + range variants)

#### Testing

- All 22 Rust tests passing
- GCS testing: 1000 objects √ó 4 sizes (4/8/16/32MB) √ó 2 modes (baseline/range)
- Performance validated on n2-standard-8 VM (same region as storage)

## [0.6.9] - 2025-10-18

### üöÄ Direct I/O Performance Fix + Clean Binary Distribution

**Critical performance fix**: 173x faster direct:// I/O through intelligent chunked reads, plus removal of redundant binaries for cleaner distribution.

#### Performance Improvements

- **Chunked reads for direct:// URIs** (files >8 MiB)
  - Before: 0.01 GiB/s (whole-file reads - CATASTROPHIC)
  - After: 1.73 GiB/s (4 MiB chunked reads - OPTIMAL)
  - **173x performance improvement** for direct:// backend
  - Zero regression for other backends (s3://, gs://, az://, file://)

- **Intelligent backend detection**
  - Automatic chunked reads for direct:// URIs only
  - Cloud storage (s3://, gs://, az://) always use whole-file (optimal for HTTP)
  - Local file:// continues with whole-file (acceptable 0.57 GiB/s)
  - Conservative 8 MiB threshold - small files use simple whole-file approach

#### Binary Distribution Cleanup

**Removed redundant binaries** for clearer user experience:
- **`sai3bench-run`** - Removed (replaced by `sai3-bench run` subcommand with more features)
- **`fs_read_bench`** - Removed (internal dev tool, not needed for production)

**Clean 3-binary distribution**:
- `sai3-bench` - Main unified CLI with subcommands: `run`, `replay`, `util`
- `sai3bench-agent` - Distributed worker
- `sai3bench-ctl` - Distributed controller

#### Infrastructure Additions

- **Async metadata pre-fetching module** (`src/metadata_prefetch.rs`)
  - Separate worker pool (8 threads default) for stat() calls
  - Eliminates metadata overhead from critical I/O path
  - Ready for future integration

#### Migration Guide

**No breaking changes** - 100% backward compatible:

Old command (removed):
```bash
sai3bench-run --config workload.yaml
```

New command (use this):
```bash
sai3-bench run --config workload.yaml
# Additional features: --dry-run, --prepare-only, --verify, --skip-prepare, --no-cleanup, --tsv-name
```

#### Technical Details

- Chunked reads use 4 MiB blocks (optimal from testing)
- Runtime safety check prevents misuse of chunked reads for non-direct:// URIs
- Graceful fallback to whole-file on metadata fetch errors
- stat() overhead negligible (<1% for local files)
- Source files for removed binaries kept in `src/bin/` and `benches/` for development reference

#### Documentation

- `docs/CHUNKED_READS_STRATEGY.md` - Comprehensive optimization strategy
- `docs/V0.6.9_RELEASE_SUMMARY.md` - Full release notes
- `benches/README.md` - Development tools guide
- Updated: `docs/USAGE.md`, `.github/copilot-instructions.md`

#### Testing

Test configs:
- `tests/configs/direct_io_chunked_test.yaml` - Validates chunked read optimization
- Test scripts in `benches/` for performance validation

---

## [0.6.8] - 2025-10-15

### üöÄ Page Cache Control for File I/O Performance

**Added configurable page cache mode** via `page_cache_mode` YAML field to optimize filesystem I/O performance through `posix_fadvise()` hints on Linux/Unix systems.

#### Features

- **YAML Configuration**: New `page_cache_mode` field accepts: `auto`, `sequential`, `random`, `dontneed`, `normal`
  - **Auto** (default): Sequential hints for files ‚â•64MB, Random for smaller files
  - **Sequential**: Optimize for sequential reads (prefetching, large readahead)
  - **Random**: Optimize for random access (minimal prefetching)
  - **DontNeed**: Drop pages from cache after read (useful for large datasets)
  - **Normal**: Default kernel behavior

- **Platform Support**:
  - Linux/Unix: Uses `posix_fadvise()` system calls via s3dlio v0.9.7+
  - Other platforms: Graceful no-op (configuration accepted but ignored)

- **Performance Impact**:
  - Sequential mode: Up to 2-3x faster for large sequential reads
  - Random mode: Better performance for small random I/O patterns
  - DontNeed: Prevents cache pollution from benchmark data

#### Integration

- Integrates with s3dlio v0.9.7+ `FileSystemConfig.page_cache_mode`
- Applies to `file://` URIs only (not S3, Azure, GCS)
- Per-operation configuration supported via YAML `workload` section

#### Testing

- Comprehensive test suite: 5 test configs + 12 automated tests
- Validates all page cache modes and backward compatibility
- Test configs in `tests/configs/page_cache_*.yaml`
- Test runner: `tests/test_page_cache_modes.sh`

#### Technical Notes

- **s3dlio API workaround**: Uses `FileSystemObjectStore::with_config()` directly due to type mismatch in `store_for_uri_with_config()` (documented in bug report)
- Dry-run mode displays page cache configuration with platform notes
- No impact on non-file:// backends (S3, Azure, GCS, direct://)

#### Example Configuration

```yaml
target: "file:///data/benchmark/"
page_cache_mode: sequential  # Apply to all operations

workload:
  - op: get
    path: "objects/*"
    weight: 100
```

See `docs/CONFIG_SYNTAX.md` for complete documentation.

---

## [0.6.7] - 2025-10-15

### üìä TSV Export Enhancement: Aggregate Summary Rows

**Added aggregate summary rows** to TSV export for easy analysis of total GET/PUT/META operations across all size buckets. Supports both single-node and distributed mode.

#### Features

- **Aggregate Rows**: Each operation type (GET, PUT, META) now includes an "ALL" summary row
  - Combines statistics across all size buckets
  - Proper HDR histogram merging for accurate latency percentiles
  - Sum of operations count and throughput across all buckets
  - Naturally sorted to end of output (META=97, GET=98, PUT=99)

- **Distributed Mode Support**:
  - Per-agent TSVs (`agents/{agent-id}/results.tsv`) include aggregate rows for that agent
  - Consolidated TSV (`results.tsv`) includes aggregate rows merged across all agents
  - HDR histogram merging ensures statistically accurate latency percentiles
  - Shows both per-agent totals and overall cluster totals
  
#### What's in the Aggregate Rows

Each "ALL" row includes:
- **Count**: Total operations across all size buckets
- **Throughput**: Total MiB/s (sum of all buckets)
- **Latency Statistics**: Properly merged HDR histogram values
  - Mean, P50, P90, P95, P99, Max latencies
  - Uses HDR histogram merge operation for accuracy
- **Average Bytes**: Weighted average object size
- **Bucket Index**: Operation-specific (META=97, GET=98, PUT=99) for natural sorting

#### Example TSV Output

```
operation  size_bucket   bucket_idx  mean_us  p50_us  p90_us   p95_us   throughput_mibps  count
GET        1B-8KiB       1           226.21   204.00  328.00   397.00   3.68              19022
GET        8KiB-64KiB    2           419.28   386.00  574.00   690.00   176.34            14258
GET        64KiB-512KiB  3           863.18   778.00  1260.00  1423.00  946.67            9568
GET        ALL           98          432.69   338.00  837.00   1038.00  1126.69           42848
PUT        8KiB-64KiB    2           137.55   120.00  186.00   222.00   14.93             4828
PUT        ALL           99          137.55   120.00  186.00   222.00   14.93             4828
```

Note: Aggregate rows use operation-specific bucket_idx (META=97, GET=98, PUT=99) ensuring they naturally sort to the end after per-bucket rows (0-8).

#### Benefits

1. **Quick Analysis**: Total GET/PUT performance at a glance without manual summation
2. **Accurate Statistics**: HDR histogram merging ensures correct latency percentiles
3. **Natural Sorting**: Operation-specific bucket_idx (META=97, GET=98, PUT=99) places aggregates at end
4. **Machine Readable**: Easy to parse and filter by bucket_idx
5. **Backward Compatible**: Existing per-bucket rows unchanged

#### Implementation Details

- New method: `TsvExporter::collect_aggregate_row()`
- Uses `OpHists::combined_histogram()` for HDR merging
- Aggregates `SizeBins` data for accurate byte counts
- Operation-specific bucket indices: META=97, GET=98, PUT=99
- All rows sorted by bucket_idx for natural ordering

---

## [0.6.6] - 2025-10-11

### ‚ö†Ô∏è BREAKING CHANGE: Command Structure Restructured

**Utility commands now nested under `util` subcommand** to emphasize core workload execution features (run/replay). This change improves tool clarity and aligns with its primary purpose as a benchmarking suite.

#### Breaking Changes
- **All utility commands require `util` prefix**:
  - ‚ùå OLD: `sai3-bench health --uri "s3://bucket/"`
  - ‚úÖ NEW: `sai3-bench util health --uri "s3://bucket/"`
  
- **Affected commands**: health, list, stat, get, put, delete
- **Unaffected commands**: run, replay (these are now prominently featured)
- **Error message**: If you try to use old syntax (e.g., `sai3-bench health`), you'll get "unrecognized subcommand" error. Simply add `util` before the command name.

#### Command Structure
```
OLD Structure:
  sai3-bench {health|list|stat|get|delete|put|run|replay}

NEW Structure (v0.6.6):
  sai3-bench {run|replay|util}
    util subcommand: {health|list|stat|get|put|delete}
```

#### Benefits
1. **Core features prominently displayed**: `run` and `replay` now appear first
2. **Cleaner help output**: Only 3 top-level commands instead of 8
3. **Clear tool purpose**: Emphasizes benchmarking/workload execution
4. **Better organization**: Utility operations clearly separated from core functionality

#### Migration Guide
Update all utility command invocations:
```bash
# Health checks
sai3-bench util health --uri "file:///tmp/test/"
sai3-bench util health --uri "s3://bucket/"
sai3-bench util health --uri "az://account/container/"
sai3-bench util health --uri "gs://bucket/"

# List operations
sai3-bench util list --uri "s3://bucket/prefix/"

# Get/Put/Delete operations
sai3-bench util get --uri "s3://bucket/*" --jobs 8
sai3-bench util put --uri "file:///tmp/" --object-size 1024 --objects 100
sai3-bench util delete --uri "s3://bucket/prefix/*"
```

#### Note on Utility Commands
For comprehensive storage CLI operations, consider using `s3-cli` from the [s3dlio package](https://github.com/russfellows/s3dlio). The utility commands in sai3-bench are provided as convenience helpers for quick testing and validation.

---

## [0.6.5] - 2025-10-11

### üîç Configuration Validation & Documentation Enhancements

**Config Validation with --dry-run**: Added comprehensive configuration validation and test summary display before execution. Users can now verify YAML syntax, validate configuration structure, and preview test details without running the workload.

#### Core Features
- **`--dry-run` Flag**: Parse and validate config with detailed summary display
  - ‚úÖ YAML syntax validation with clear error messages
  - ‚úÖ Configuration structure and required fields verification
  - ‚úÖ Test summary with duration, concurrency, backend detection
  - ‚úÖ Prepare phase details (size distributions, fill patterns, dedup/compress factors)
  - ‚úÖ Workload operations with weight percentages and concurrency overrides
  - ‚úÖ RangeEngine configuration display (when enabled)
  
- **Data Generation Documentation**: Created comprehensive DATA_GENERATION.md guide
  - Fill pattern recommendations: **Use `fill: random` for realistic storage testing**
  - Clear explanation: `fill: zero` produces artificially high compression ratios
  - Deduplication and compression factor documentation with examples
  - Implementation details showing exact Rust code paths

- **Documentation Cleanup**: Removed outdated WARP_PARITY_STATUS.md
  - File was feature-planning focused and out of date
  - References removed from README.md and docs/README.md

#### Usage Examples
```bash
# Validate config before running
sai3-bench run --config my-workload.yaml --dry-run

# Example output shows:
# - Test configuration (duration, concurrency, backend)
# - Prepare phase details (if configured)
# - Workload operations with percentages
# - Configuration validation status
```

#### Files Changed
- `src/main.rs` - Added `--dry-run` flag and `display_config_summary()` function
- `docs/DATA_GENERATION.md` - NEW: Comprehensive data generation guide (165 lines)
- `docs/CONFIG_SYNTAX.md` - Added configuration validation section with examples
- `README.md` - Added --dry-run to Quick Start, updated documentation links
- `docs/README.md` - Updated to v0.6.5, added DATA_GENERATION.md reference
- `docs/WARP_PARITY_STATUS.md` - REMOVED (outdated)

#### Testing
- Validated with `tests/configs/mixed.yaml` - Basic GET/PUT operations
- Validated with `tests/configs/size_distributions_test.yaml` - Prepare phase with distributions
- Validated with `tests/configs/per_op_concurrency_test.yaml` - Per-operation concurrency
- Validated with `tests/configs/dedupe_compress_test.yaml` - Random fill with dedup/compress
- Error handling verified with invalid YAML and incomplete configs

## [0.6.4] - 2025-10-11

### üéØ Enhanced Output with HDR Histogram Merging

**Automatic Results Directories & Consolidated Metrics**: Implemented automatic results directory creation for both single-node and distributed workloads, with HDR histogram merging for mathematically accurate aggregate metrics across multiple agents.

#### Core Features
- **Timestamped Results Directories**: Automatic creation of `sai3-YYYYMMDD-HHMM-{test_name}/` directories
- **Complete Capture**: config.yaml, console.log, metadata.json, results.tsv in every results directory
- **Distributed Results Collection**: Per-agent results in `agents/{id}/` subdirectories with metadata
- **HDR Histogram Merging**: Mathematically accurate percentile aggregation across multiple agents

#### Results Directory Structure
```
sai3-YYYYMMDD-HHMM-{test_name}/
‚îú‚îÄ‚îÄ config.yaml                    # Controller's workload config
‚îú‚îÄ‚îÄ console.log                    # Complete execution log
‚îú‚îÄ‚îÄ metadata.json                  # Test metadata (distributed: true/false)
‚îú‚îÄ‚îÄ results.tsv                    # Single-node OR consolidated (merged histograms)
‚îî‚îÄ‚îÄ agents/                        # Only in distributed mode
    ‚îú‚îÄ‚îÄ agent-1/
    ‚îÇ   ‚îú‚îÄ‚îÄ metadata.json          # Agent-specific metadata
    ‚îÇ   ‚îú‚îÄ‚îÄ results.tsv           # Agent-1 individual results
    ‚îÇ   ‚îî‚îÄ‚îÄ agent_local_path.txt  # Points to agent's /tmp/ directory
    ‚îî‚îÄ‚îÄ agent-2/...
```

#### HDR Histogram Merging (Critical Enhancement)
**Problem**: Percentiles cannot be simply averaged across agents. If agent-1 has p95=278¬µs and agent-2 has p95=280¬µs, the aggregate p95 is NOT necessarily 279¬µs.

**Solution**: Proper histogram merging via `hdrhistogram` library:
- Agent serializes 9 size-bucketed histograms per operation type (GET/PUT/META)
- Controller deserializes and merges histograms mathematically
- Consolidated results.tsv contains accurate aggregate percentiles
- Per-agent results preserved for debugging

**Performance Impact**: Negligible overhead (~1ms agent serialization, ~3ms controller merge for 2 agents)

#### gRPC Protocol Extension
Extended `WorkloadSummary` message with histogram fields:
- `histogram_get`, `histogram_put`, `histogram_meta` (bytes) - V2 binary format
- Efficient transfer: Few KB per agent
- Full backward compatibility maintained

#### Files Changed
- `proto/iobench.proto` - Extended with histogram fields
- `src/bin/agent.rs` - Histogram serialization (+109 lines)
- `src/bin/controller.rs` - Histogram merging + consolidated TSV generation (+250 lines)
- `src/pb/iobench.rs` - Auto-generated protobuf code

#### Testing
- Comprehensive test suite with 4 test scenarios
- Verified 2-agent, 4-agent, and multi-size workloads
- All tests passing with exact count verification
- Histogram accuracy validated (proper merging, not averaging)

**Test Scripts**:
- `tests/verify_v0.6.4.sh` - Quick 2-agent verification
- `tests/test_comprehensive_v0.6.4.sh` - Full test suite
- `tests/configs/distributed_mixed_test.yaml` - Mixed workload config

#### Migration Notes
No breaking changes - fully backward compatible. Results directories are created automatically for all workload executions.

## [0.6.3] - 2025-10-10

### üéØ Critical Performance Fix: s3dlio v0.9.6 Upgrade

**Resolves 20-25% Performance Regression**: Upgraded to s3dlio v0.9.6 which disables RangeEngine by default across all backends, eliminating HEAD request overhead that caused significant slowdowns in typical workloads.

#### Core Library Upgrade
- **s3dlio**: v0.9.5 ‚Üí **v0.9.6** (git tag)
- **s3dlio-oplog**: v0.9.5 ‚Üí **v0.9.6** (git tag)

#### Performance Impact
**Problem Identified**: s3dlio v0.9.5 enabled RangeEngine by default, which added a HEAD/STAT request before every GET operation to determine object size. This caused:
- **20-25% throughput degradation** for typical workloads with mixed object sizes
- **60% more requests** for small-object workloads (HEAD + GET vs just GET)
- HEAD overhead exceeds RangeEngine benefits for objects < 64 MiB

**Solution**: s3dlio v0.9.6 disables RangeEngine by default for ALL backends:
- `AzureConfig::default()`: `enable_range_engine = false`
- `GcsConfig::default()`: `enable_range_engine = false`
- `FileSystemConfig::default()`: `enable_range_engine = false`
- `DirectIOConfig::default()`: `enable_range_engine = false`

**Performance Validation** (GCS with 64 MiB objects):
- **Disabled (v0.9.6 default)**: 53.85 MiB/s GET throughput
- **Enabled (explicit opt-in)**: 54.47 MiB/s GET throughput (+1.2%)
- **Conclusion**: Minimal benefit when network bandwidth is the bottleneck

#### Configuration Changes
**Default Behavior** (v0.6.3):
- RangeEngine **DISABLED** by default (via s3dlio v0.9.6)
- Optimal for typical workloads with mixed object sizes
- Single GET request per operation (no HEAD overhead)

**Explicit Opt-In** (for large-file workloads ‚â• 64 MiB):
```yaml
range_engine:
  enabled: true
  min_split_size: 16777216  # 16 MiB threshold
  chunk_size: 67108864      # 64 MiB chunks
  max_concurrent_ranges: 16
```

#### Enhanced Logging
**RangeEngine Status Visibility**: Added clear logging for all backends:
```
INFO sai3_bench: RangeEngine DISABLED for Google Cloud Storage backend (default for optimal performance)
INFO sai3_bench: RangeEngine ENABLED for Google Cloud Storage backend - files >= 16 MiB
```

**Applies to**: File, DirectIO, S3, Azure, and GCS backends

#### Testing & Validation
**Comprehensive Testing**:
- ‚úÖ File backend: Confirmed disabled by default (940.83 MiB/s GET)
- ‚úÖ GCS with 1 MiB objects: 36.83 ops/s baseline performance
- ‚úÖ GCS with 64 MiB objects: 53.85 MiB/s disabled vs 54.47 MiB/s enabled
- ‚úÖ Network bandwidth validation: Real GCS testing over 500 Mb/s connection

**Key Insight**: RangeEngine provides benefit only when:
1. Objects are large (‚â• 64 MiB)
2. Network bandwidth is NOT the bottleneck (10+ Gbps)
3. Parallel range downloads can overcome other bottlenecks

For typical cloud storage workloads, disabled is optimal.

#### Breaking Changes
**None for most users**: Default behavior now faster for typical workloads.

**Large-file workloads only**: Users relying on automatic RangeEngine for large files must explicitly enable it in configuration.

#### Modified Files
- `Cargo.toml`: Updated s3dlio dependencies to v0.9.6
- `Cargo.lock`: Locked to s3dlio v0.9.6 commit hash
- `src/config.rs`: Updated documentation for disabled default
- `src/main.rs`: Enhanced RangeEngine status logging for all backends

---

## [0.6.1] - 2025-10-10

### üöÄ Major Upgrade: s3dlio v0.9.4 with RangeEngine

**Breaking Through Performance Limits**: Upgraded to s3dlio v0.9.4 with RangeEngine support, providing 30-50% throughput improvements for large files on network backends.

#### Core Library Upgrade
- **s3dlio**: v0.8.22 (git main) ‚Üí **v0.9.4** (stable git tag)
- **s3dlio-oplog**: v0.8.22 (git main) ‚Üí **v0.9.4** (stable git tag)
- **Pinned to stable release**: Using git tag instead of branch for production stability

#### RangeEngine Technology
**What is RangeEngine?**: Introduced in s3dlio v0.9.3, RangeEngine dramatically improves download performance for large files by using concurrent byte-range requests. Instead of downloading a 128MB file sequentially, RangeEngine downloads it in parallel 64MB chunks.

**Performance Gains** (from comprehensive testing):
- **Google Cloud Storage**: 45.26 ops/s, 173ms mean latency
- **Azure Blob Storage**: 8.46 ops/s, 912ms mean latency  
- **GCS vs Azure**: **5.3x faster** with RangeEngine on GCS
- **Activation**: Automatic for files ‚â• 4MB
- **Scaling**: 128MB files use 2 concurrent ranges, 256MB use 4 ranges

**Key Findings**:
- ‚úÖ 8MB files: 1 range activation confirmed on Azure & GCS
- ‚úÖ 128MB files: 2 ranges activation confirmed on Azure & GCS
- ‚úÖ File backend: RangeEngine activates but shows limited benefit (already fast local I/O)
- ‚úÖ Network backends: 30-50% throughput improvement for large files

#### Agent Modernization: Universal Backend Support
**Problem**: Agent previously used S3-specific `s3_utils` functions, limiting it to S3 backend only.

**Solution**: Complete migration to universal `ObjectStore` pattern:
- **Before**: `get_object()`, `put_object_async()` from s3_utils (S3-only)
- **After**: `store_for_uri()` with ObjectStore trait (all backends)
- **Benefit**: Agent now supports file://, direct://, s3://, az://, gs:// URIs
- **Automatic RangeEngine**: All network operations benefit from RangeEngine

**Modified**: `src/bin/agent.rs`
- Removed: S3-specific imports
- Added: `store_for_uri()` for universal backend support
- Refactored: `run_get()` and `run_put()` to use ObjectStore pattern
- New helper: `list_keys_for_uri()` for multi-backend listing

#### API Compatibility Fix
**s3dlio v0.9.4 API Change**: `ObjectStore::get()` now returns `bytes::Bytes` instead of `Vec<u8>`.

**Fix**: Added `.to_vec()` conversions in workload operations:
- `src/workload.rs`: `get_object_multi_backend()` line 513
- `src/workload.rs`: `get_object_no_log()` lines 584-585

**Performance Impact**: Minimal - single memory copy, maintains compatibility.

#### RangeEngine Configuration
**New**: `RangeEngineConfig` structure in `src/config.rs` for future configurability:
```yaml
# Optional in workload configs (uses s3dlio defaults if omitted)
range_engine:
  enabled: true
  chunk_size: 67108864      # 64 MB
  max_concurrent_ranges: 32
  min_split_size: 4194304   # 4 MB threshold
  range_timeout_secs: 30
```

**Current Status**: Documentary only - s3dlio uses optimal defaults automatically. Configuration exposed for advanced tuning if needed.

#### TSV Output Improvement
**Enhancement**: TSV benchmark results now sorted by `bucket_idx` for better readability.

**Before**: Rows grouped by operation (GET, PUT, META) with mixed bucket order  
**After**: All rows sorted by bucket_idx (0 ‚Üí 8), making size-bucket analysis intuitive

**Example**:
```
operation  size_bucket      bucket_idx  mean_us  ...
GET        1B-8KiB          1          2347.39   ...
PUT        1B-8KiB          1          250.55    ...
GET        64KiB-512KiB     3          13419.45  ...
PUT        64KiB-512KiB     3          360.61    ...
```

**Modified**: `src/tsv_export.rs`

### üìä Comprehensive Testing

#### Backend Coverage
- ‚úÖ **File backend**: 11,188 ops/s, 19GB workload tested
- ‚úÖ **Azure Blob Storage**: 8.46 ops/s, RangeEngine confirmed (1-4 ranges)
- ‚úÖ **Google Cloud Storage**: 45.26 ops/s, RangeEngine confirmed (1-2 ranges)

#### Quality Metrics
- ‚úÖ Unit tests: 18/18 passing
- ‚úÖ Integration tests: 1/1 passing
- ‚úÖ Distributed workload: Tested and working
- ‚úÖ No regressions detected
- ‚úÖ Zero compilation warnings

### üìö New Documentation
- **`docs/S3DLIO_V0.9.4_MIGRATION.md`**: Complete 400+ line migration guide
  - All changes explained with examples
  - Deprecation analysis
  - RangeEngine technical deep-dive
  - Performance expectations and tuning
- **`docs/S3DLIO_V0.9.4_TEST_RESULTS.md`**: Comprehensive test results
  - Per-backend performance metrics
  - RangeEngine validation details
  - Latency percentiles
- **`docs/S3DLIO_V0.9.4_TESTING_PLAN.md`**: Testing methodology and matrix
- **`tests/configs/README.md`**: Complete guide to all test configurations
  - Quick reference table
  - Usage examples
  - Environment setup
  - Troubleshooting

### üß™ New Test Configurations
- **`tests/configs/gcs_rangeengine_test.yaml`**: GCS RangeEngine performance test
- **`tests/configs/azure_rangeengine_test.yaml`**: Azure RangeEngine test (full)
- **`tests/configs/azure_rangeengine_simple.yaml`**: Azure minimal example
- **`tests/configs/azure_rangeengine_disabled_test.yaml`**: Baseline comparison
- **`tests/configs/range_engine_test.yaml`**: File backend RangeEngine demo

### üîß Technical Details

#### Files Modified (13 total)
**Core Code** (5 files):
- `Cargo.toml` - Dependencies and version
- `src/workload.rs` - Bytes compatibility
- `src/bin/agent.rs` - ObjectStore migration
- `src/config.rs` - RangeEngine config structure
- `src/tsv_export.rs` - Output sorting

**Test Configs** (6 files):
- 3 modified: Fixed SizeSpec syntax
- 3 created: New RangeEngine examples

**Documentation** (3 files):
- All new comprehensive guides

#### Deprecation Status
**Clean**: sai3-bench does NOT use any deprecated s3dlio APIs:
- ‚úÖ `get_object()` - Still supported
- ‚úÖ `put_object_async()` - Still supported
- ‚úÖ `parse_s3_uri()` - Still supported
- ‚ö†Ô∏è `list_objects()` - Deprecated, but we don't use it

### üéØ Performance Summary

| Backend | Throughput | Latency | RangeEngine | Relative |
|---------|------------|---------|-------------|----------|
| **GCS** | 45.26 ops/s | 173ms | ‚úÖ 1-2 ranges | **5.3x faster** |
| **File** | 11,188 ops/s | <1ms | ‚úÖ Activated | Baseline |
| **Azure** | 8.46 ops/s | 912ms | ‚úÖ 1-4 ranges | Expected |

### üöÄ Upgrade Path

**Backward Compatible**: No breaking changes to user-facing APIs.

**Migration Steps**:
1. Update dependency: `sai3-bench = "0.6.1"`
2. Rebuild: `cargo build --release`
3. Test: `sai3-bench run --config your-config.yaml`
4. Optional: Review RangeEngine logs with `-vv` flag

**See Also**: `docs/S3DLIO_V0.9.4_MIGRATION.md` for detailed migration guide.

### üôè Acknowledgments
- s3dlio maintainers for RangeEngine implementation
- Testing on Google Cloud Platform and Azure

---

## [0.6.0] - 2025-10-07

### üöÄ Major Features

#### Distributed Multi-Host Workload Execution
**New Capability**: Run coordinated benchmarks across multiple agent nodes using gRPC.

**Problem**: Previous versions could only run single-node benchmarks. Testing distributed systems at scale required manual coordination.

**Solution**: Added complete distributed workload infrastructure:
- **New gRPC RPC**: `RunWorkload` - Controller sends config to agents for execution
- **Per-Agent Path Isolation**: Each agent operates in isolated subdirectory (e.g., `agent-1/`, `agent-2/`)
- **Coordinated Start Time**: All agents begin workload simultaneously (configurable delay)
- **Result Aggregation**: Controller collects and displays per-agent and aggregate statistics
- **Shared vs Local Storage Detection**: Automatic handling based on URI scheme
  - **Shared storage** (S3/GCS/Azure): All agents use same prepared dataset
  - **Local storage** (file://): Each agent prepares own isolated dataset

**New Components**:
- `sai3bench-ctl run` subcommand - Distributed workload orchestration
- `RunWorkloadRequest` protobuf message - Config distribution with agent metadata
- `WorkloadSummary` protobuf message - Per-agent execution results
- `Config::apply_agent_prefix()` - Path isolation with storage-aware prepare handling

**Usage**:
```bash
# Start agents on multiple hosts
sai3bench-agent --listen 0.0.0.0:7761  # On host 1
sai3bench-agent --listen 0.0.0.0:7761  # On host 2

# Run distributed workload from controller
sai3bench-ctl --insecure --agents host1:7761,host2:7761 \
    run --config workload.yaml --start-delay 2

# Output shows per-agent and aggregate results
=== Distributed Results ===
Total agents: 2

--- Agent: agent-1 ---
  Wall time: 3.03s
  Total ops: 30966 (10215.21 ops/s)
  Total bytes: 25.67 MB (8.47 MiB/s)
  GET: 21602 ops, 21.10 MB, mean: 225¬µs, p95: 315¬µs
  PUT: 9364 ops, 4.57 MB, mean: 109¬µs, p95: 155¬µs

--- Agent: agent-2 ---
  Wall time: 3.03s
  Total ops: 30868 (10179.66 ops/s)
  Total bytes: 25.63 MB (8.45 MiB/s)
  GET: 21630 ops, 21.12 MB, mean: 225¬µs, p95: 319¬µs
  PUT: 9238 ops, 4.51 MB, mean: 108¬µs, p95: 153¬µs

--- Aggregate ---
  Total ops: 61834
  Total bytes: 51.30 MB
  Combined throughput: 20394.87 ops/s
```

**Controller Flags**:
- `--config <file>` - YAML workload configuration file
- `--path-template <template>` - Agent path prefix template (default: `agent-{id}/`)
- `--agent-ids <list>` - Custom agent identifiers (default: `agent-1`, `agent-2`, ...)
- `--start-delay <seconds>` - Coordinated start delay (default: 2)
- `--shared-prepare` - Override auto-detected storage mode

**Technical Details**:
- Auto-detection based on URI scheme:
  - Shared: `s3://`, `az://`, `gs://`
  - Local: `file://`, `direct://`
- Prepare config modification only for local storage
- Target URI always gets agent prefix for workload isolation
- Operation paths remain relative (resolve against modified target)
- Each agent executes prepare phase independently if needed
- Coordinated start time uses nanosecond-precision timestamp

**Testing**:
- `tests/distributed_local_test.sh` - Integration test with file:// backend
- Verified with 2 agents creating isolated datasets (10 prepare objects each)
- Confirmed path isolation (agent-1/ and agent-2/ subdirectories)
- Validated result aggregation and throughput calculations

**Documentation**:
- `docs/V0.6.0_DISTRIBUTED_DESIGN.md` - Complete design document
- `docs/USAGE.md` - Updated with distributed examples (TBD)

### üîß Technical Changes
- Extended `proto/iobench.proto` with `RunWorkload` RPC
- Added `shared_storage` field to `RunWorkloadRequest`
- Implemented `Agent::run_workload()` handler with prepare phase
- Added storage detection helpers in controller
- Enhanced logging with `-v`/`-vv` flags in controller and agent
- Fixed prepare `base_uri` rewriting for path isolation

### üìä Performance Characteristics
- Tested with 2 agents on localhost (file:// backend)
- Each agent: ~10,000 ops/s, ~25 MB in 3 seconds
- Combined throughput: ~20,000 ops/s
- No synchronization overhead (agents run independently)
- Coordinated start ensures fair comparison

## [0.5.9] - 2025-10-07

### ‚ú® Improvements

#### Output Organization & Clarity
**Problem**: Console output had duplication and missing metrics:
- Latency histograms shown separately, then repeated in Results section
- TSV export message shown twice (with and without checkmark)
- Mean/average latency was missing from Results output (only p50/p95/p99 shown)

**Solution**: Consolidated and enhanced Results output:
- **Added mean latency**: Now shows `Latency mean: X¬µs, p50: X¬µs, p95: X¬µs, p99: X¬µs` for all operations
- **Removed duplicate histogram**: Detailed histograms removed from workload output (was redundant)
- **Removed duplicate TSV message**: Single export confirmation with checkmark emoji
- **Cleaner flow**: Results section now shows all key metrics in one organized place

#### Branding Consistency
**Problem**: Some references still used legacy "io-bench" terminology instead of "sai3-bench".

**Solution**: Fixed all remaining references:
- Updated CLI help examples in `sai3-bench put`, `run`, and `replay` commands
- Updated code comments in replay.rs and test files
- Consistent branding across all user-facing messages

### üîß Technical Changes
- Added `mean_us` field to `OpAgg` struct
- Calculate mean from HDR histograms using `hist.mean()`
- Display mean alongside percentiles in Results output
- Code comment updates for branding consistency

### üìä Example Output
```
=== Results ===
Wall time: 3.03s
Total ops: 71317
Total bytes: 102281216 (97.54 MB)
Throughput: 23507.30 ops/s

GET operations:
  Ops: 42750 (14091.13 ops/s)
  Bytes: 43776000 (41.75 MB)
  Throughput: 13.76 MiB/s
  Latency mean: 181¬µs, p50: 175¬µs, p95: 273¬µs, p99: 338¬µs

PUT operations:
  Ops: 28567 (9416.17 ops/s)
  Bytes: 58505216 (55.79 MB)
  Throughput: 18.39 MiB/s
  Latency mean: 92¬µs, p50: 84¬µs, p95: 143¬µs, p99: 193¬µs

‚úÖ TSV results exported to: sai3bench-2025-10-07-150959-test_mean_output-results.tsv
```

---

## [0.5.8] - 2025-10-07

### üêõ Bug Fix

#### GCS Pagination Fix (via s3dlio v0.8.22)
**Problem**: Google Cloud Storage (GCS) list and delete operations were limited to 1,000 objects due to missing pagination handling in s3dlio v0.8.21 and earlier.

**Solution**: Updated to s3dlio v0.8.22 which implements proper pagination:
- List operations now retrieve all objects (not just first 1,000)
- Delete operations now remove all matched objects (not just first 1,000)
- Operations process in batches of 1,000 as per GCS API limits
- Affects `list`, `delete`, and glob pattern operations on GCS

**Impact**: 
- **Critical for GCS users with >1,000 objects**: Previous versions silently failed to process beyond first page
- **No impact on other backends**: S3 and Azure already had correct pagination
- **Workload prepare**: Now correctly deletes all objects during cleanup phase
- **DELETE operations**: Now remove all matched objects, not just first 1,000

### üîß Technical Changes
- Updated `s3dlio` dependency: v0.8.21 ‚Üí v0.8.22 (main branch)
- Updated `s3dlio-oplog` dependency: v0.8.21 ‚Üí v0.8.22 (main branch)
- Using `branch = "main"` until v0.8.22 tag is created in s3dlio repo

### üìö Recommendation
- **GCS users**: Upgrade immediately if working with >1,000 objects
- **Other users**: Optional upgrade, but recommended for latest fixes

---

## [0.5.7] - 2025-10-07

### üî• Critical Bug Fix

#### DELETE Pool Corruption Fixed
**Problem**: In v0.5.6 and earlier, all operations (GET, STAT, DELETE) shared a single object pool. DELETE operations removed objects during execution, causing GET/STAT to fail with 404 errors in mixed workloads.

**Solution**: Implemented automatic separate object pools (MinIO Warp approach):
- **Readonly pool** (`prepared-*.dat`): Used by GET and STAT operations, never deleted
- **Deletable pool** (`deletable-*.dat`): Used by DELETE operations, consumed during test
- **Automatic detection**: Detects mixed workloads (DELETE + GET/STAT) and creates separate pools
- **Pattern rewriting**: Transparently rewrites patterns to route operations to correct pools
- **100% backward compatible**: Single pool created when no DELETE or no GET/STAT operations

Example:
```yaml
workload:
  - op: get
    path: "prepared-*.dat"
    weight: 60
  - op: delete
    path: "prepared-*.dat"  # Auto-rewritten to deletable-*.dat
    weight: 20
  - op: stat
    path: "prepared-*.dat"
    weight: 20
```

Console output:
```
Mixed workload: Using separate object pools (readonly for GET/STAT, deletable for DELETE)
Prepared 100 objects
  50 prepared-*.dat (readonly pool)
  50 deletable-*.dat (consumable pool)
```

**Impact**: Eliminates 404 errors in mixed workloads. All users running DELETE operations should upgrade immediately.

### ‚ú® New Features

#### Automatic TSV Export with Smart Naming
**Previous behavior**: TSV export required `--results-tsv` flag (easy to forget)

**New behavior**: TSV export is automatic and mandatory with intelligent naming:

```bash
# Automatic timestamp-based naming
sai3-bench run --config test.yaml
  ‚Üí Creates: sai3bench-2025-10-07-143052-test-results.tsv

# Custom naming
sai3-bench run --config test.yaml --tsv-name my-benchmark
  ‚Üí Creates: my-benchmark-results.tsv
```

**Filename format**: `sai3bench-YYYY-MM-DD-HHMMSS-<config_basename>-results.tsv`
- Ensures unique files for repeated runs
- Easy identification of which config was used
- Auto-ignored via `.gitignore` pattern

**Breaking change**: `--results-tsv` flag removed (use `--tsv-name` for custom naming)

#### Comprehensive Throughput Reporting
Added MiB/s throughput and per-operation ops/s to console output:

```
=== Results ===
Wall time: 10.10s
Total ops: 313303
Total bytes: 1933875200 (1844.29 MB)
Throughput: 31011.97 ops/s

GET operations:
  Ops: 188855 (18693.61 ops/s)          # NEW: Per-operation ops/s
  Bytes: 1933875200 (1844.29 MB)
  Throughput: 182.55 MiB/s              # NEW: Actual data throughput!
  Latency p50: 390¬µs, p95: 597¬µs, p99: 717¬µs

PUT operations:
  Ops: 7371 (488.29 ops/s)
  Bytes: 481286288 (458.99 MB)
  Throughput: 30.41 MiB/s               # NEW: Write throughput!
  Latency p50: 369¬µs, p95: 1887¬µs, p99: 5215¬µs
```

Formula: `MiB/s = (bytes / 1,048,576) / wall_seconds`

Matches TSV export `throughput_mibps` column for consistency.

### üìù Configuration Examples

#### New Test Configurations
- `tests/configs/v057_mixed_workload_test.yaml` - Tests automatic pool separation
- `tests/configs/v057_readonly_only_test.yaml` - Tests backward compatibility (single pool)
- `tests/configs/v057_delete_only_test.yaml` - Tests DELETE-only workload
- `tests/configs/comprehensive_test.yaml` - All operations with varied sizes

### üîß Technical Changes

**Modified Files**:
- `src/workload.rs`: Pool separation logic, pattern rewriting, prepare enhancement
- `src/main.rs`: TSV auto-export, `--tsv-name` flag, throughput reporting
- `.gitignore`: Added `sai3bench-*.tsv` pattern
- `Cargo.toml`: Version 0.5.6 ‚Üí 0.5.7

**New Functions**:
- `detect_pool_requirements()`: Analyzes workload for pool needs
- `rewrite_pattern_for_pool()`: Rewrites patterns for correct pool routing

### üìö Documentation
- Added `docs/V0.5.7_RELEASE_SUMMARY.md` (comprehensive release notes)
- Updated `docs/CHANGELOG.md` (this file)
- Updated `README.md` (brief v0.5.7 mention)

### ‚ö†Ô∏è Breaking Changes
- Removed `--results-tsv` flag (use `--tsv-name` for custom naming, or rely on automatic naming)

### üéØ Migration Guide

**From v0.5.6**:
1. Remove `--results-tsv` from scripts
2. Optionally add `--tsv-name <basename>` for custom naming
3. Mixed workloads with DELETE now work correctly (no config changes needed!)

**Backward Compatibility**:
- All YAML configs work unchanged
- Readonly-only workloads use single pool (no overhead)
- DELETE-only workloads use single pool (no overhead)
- Mixed workloads automatically get separate pools (fixes 404 errors)

---

## [0.5.6] - 2025-10-07

### ÔøΩ New Features

#### Configurable Post-Prepare Delay
**Problem**: Cloud storage backends (GCS, S3, Azure) have eventual consistency. Objects created during the prepare phase might not be immediately readable, causing 404 errors when the workload starts.

**Solution**: Added YAML-configurable delay between prepare and workload phases:
```yaml
prepare:
  post_prepare_delay: 5  # Wait 5 seconds after creating objects
  ensure_objects:
    - base_uri: "gs://bucket/data/"
      count: 200
```

**Recommendations**:
- Local storage (`file://`, `direct://`): 0 seconds (default)
- Cloud storage (S3, GCS, Azure): 2-5 seconds
- Large object counts (>1000): 5-10 seconds

#### Manual Phase Execution
Added CLI flags for manual control of benchmark phases:

**`--verify`**: Verify prepared objects exist and are accessible
```bash
# Step 1: Prepare objects
sai3-bench run --config test.yaml --prepare-only

# Step 2: Wait for propagation (manual)
sleep 60

# Step 3: Verify all objects are accessible
sai3-bench run --config test.yaml --verify

# Step 4: Run workload (skip prepare since objects exist)
sai3-bench run --config test.yaml --skip-prepare
```

**`--skip-prepare`**: Skip prepare phase, assume objects already exist
```bash
# First run: prepare and run
sai3-bench run --config test.yaml --no-cleanup

# Subsequent runs: skip prepare, reuse existing objects
sai3-bench run --config test.yaml --skip-prepare
```

**Verification Output**:
```
=== Verification Phase ===
Verifying objects at gs://bucket/data/
‚úì 200/200 objects verified and accessible at gs://bucket/data/
```

### üêõ Bug Fixes

#### Cloud Storage Eventual Consistency Protection
**Impact**: Benchmarks on cloud storage failed with "404 Not Found" errors immediately after prepare completed.

**Root Cause**: When prepare created objects very quickly (>300 objects/sec), cloud storage eventual consistency meant objects weren't immediately readable in all zones.

**Example Error**:
```
Error: Failed to get object from URI: gs://bucket/prepared-00000117.dat
Caused by:
    GCS GET failed: HTTP status client error (404 Not Found)
```

**Fix**: 
- Added `post_prepare_delay` field to `PrepareConfig`
- Delay only applies if new objects were created (not if they already existed)
- Users have full control via YAML configuration
- Manual workflow supported via `--verify` flag

**Files Changed**: 
- `src/config.rs` - Added `post_prepare_delay` field
- `src/main.rs` - Added `--verify` and `--skip-prepare` flags, configurable delay
- `src/workload.rs` - Added `verify_prepared_objects()` function

### üìö Documentation

- Added `examples/cloud-storage-with-delay.yaml` - Complete example with phased execution
- Updated `docs/CONFIG_SYNTAX.md` - Documented `post_prepare_delay` field and recommendations
- Added detailed CLI usage examples for phased workflows

### üîÑ Migration Guide

**Before (v0.5.5)**:
```yaml
prepare:
  ensure_objects:
    - base_uri: "gs://bucket/data/"
      count: 200
```
Objects created, workload started immediately ‚Üí 404 errors on cloud storage

**After (v0.5.6)**:
```yaml
prepare:
  post_prepare_delay: 3  # Add this line for cloud storage
  ensure_objects:
    - base_uri: "gs://bucket/data/"
      count: 200
```
Objects created, waits 3 seconds, workload starts ‚Üí no errors

**No changes required for local storage** (`file://`, `direct://`) - default delay is 0.

## [0.5.5] - 2025-10-06

### üöÄ Critical Performance & Correctness Fixes

This release fixes **critical bugs** that prevented DELETE and STAT operations from working with glob patterns, and adds **parallel execution** to prepare and cleanup stages for 30x performance improvement.

### üêõ Critical Bug Fixes

#### Pattern Resolution for DELETE and STAT Operations
**Problem**: DELETE and STAT operations were completely broken when using glob patterns. They attempted to delete/stat the pattern string itself (e.g., `prepared-*.dat`) instead of resolving it to actual object URIs.

**Impact**: 
- DELETE operations always failed with "No such object" errors
- STAT operations always failed with "No such object" errors
- Mixed workloads (similar to MinIO Warp benchmarks) were impossible to run
- Cloud storage benchmarking was severely limited

**Fix**: Extended pre-resolution logic to handle DELETE and STAT operations:
- All three operations (GET, DELETE, STAT) now pre-resolve glob patterns at startup
- Workers randomly sample from pre-resolved URI lists during execution
- Added `UriSource` tracking for each operation type
- User sees: `Resolving 3 operation patterns (1 GET, 1 DELETE, 1 STAT)...`

**Files Changed**: `src/workload.rs` (lines 903-927, 563-650, 810-850)

**Example**:
```yaml
workload:
  - op: delete
    path: "bench/mixed/prepared-*.dat"  # ‚úÖ Now works - resolves to actual objects
```

See: `docs/PATTERN_RESOLUTION_FIX.md` for complete details

### ‚ö° Performance Improvements

#### Parallel Prepare Stage (30x faster)
**Problem**: Prepare stage created objects sequentially (one at a time), making cloud storage preparation extremely slow.

**Impact Before**:
- 20,000 objects took ~35-50 seconds
- Throughput: ~400-600 objects/sec
- Cloud benchmarking required long waits before tests could begin

**Fix**: Implemented parallel execution with 32 workers using semaphore-controlled concurrency:
- Uses same pattern as main workload execution
- Pre-generates URIs and sizes, then executes in parallel with `FuturesUnordered`
- Semaphore limits concurrent tasks to prevent resource exhaustion

**Impact After**:
- Small objects (100 KiB): **13,179-18,677 objects/sec** (30x improvement)
- Large objects (1 MiB): **788 objects/sec at 788 MB/sec throughput**
- 20,000 objects (1 MiB): **25.4 seconds** vs 50+ seconds

**Files Changed**: `src/workload.rs` (lines 135-197)

#### Parallel Cleanup Stage (30x faster)
**Problem**: Cleanup stage deleted objects sequentially (one at a time), causing slow cleanup after benchmarks.

**Fix**: Applied same parallel execution pattern as prepare stage:
- 32 parallel workers with semaphore control
- Graceful error handling (logs warnings but continues on single delete failures)
- Best-effort deletion approach

**Impact**:
- 2,000 objects: **< 0.2 seconds** (was ~3-5 seconds)
- 5,000 objects: **< 0.3 seconds** (was ~7-12 seconds)
- Throughput: **>10,000 objects/sec** for small objects

**Files Changed**: `src/workload.rs` (lines 236-310)

See: `docs/PREPARE_PERFORMANCE_FIX.md` for complete benchmarking results

### üìù Configuration Syntax Updates

#### Glob Patterns (Not Brace Expansions)
sai3-bench uses **glob patterns with wildcards**, not bash-style brace expansions:

**Correct** ‚úÖ:
```yaml
workload:
  - op: get
    path: "bench/mixed/prepared-*.dat"  # Glob pattern with wildcard
```

**Incorrect** ‚ùå:
```yaml
workload:
  - op: get
    path: "bench/mixed/obj_{00000..19999}"  # Brace expansion NOT supported
```

#### Object Naming in Prepare Stage
Prepare stage creates objects with this naming pattern:
- Format: `prepared-NNNNNNNN.dat` (8-digit zero-padded with `.dat` extension)
- Example: `prepared-00000000.dat`, `prepared-00000001.dat`, etc.

Match your workload patterns accordingly:
```yaml
prepare:
  ensure_objects:
    - base_uri: "gs://bucket/data/"
      count: 1000

workload:
  - op: get
    path: "data/prepared-*.dat"  # ‚úÖ Matches prepare naming
```

### üìö Documentation Improvements

#### New Documentation
- `docs/PATTERN_RESOLUTION_FIX.md` - Complete guide to pattern resolution fix
- `docs/PREPARE_PERFORMANCE_FIX.md` - Performance improvements with benchmarks (updated for cleanup)
- `examples/README.md` - Comprehensive guide to all operation types
- `examples/mixed-workload-cloud.yaml` - Production-ready cloud benchmark example
- `examples/all-operations.yaml` - Demonstrates all 5 operation types

#### Updated Examples
- Moved YAML examples from `docs/` to `examples/` directory
- All examples now use correct glob pattern syntax
- Added detailed comments explaining each operation type
- Included weight balancing best practices

### üß™ Testing
- Added regression tests in `tests/configs/prepare-performance/`
- Added regression tests in `tests/configs/cleanup-performance/`
- Added pattern resolution test: `tests/configs/pattern-resolution-test.yaml`
- Validated all operation types work correctly with glob patterns

### üîß Technical Details

**Concurrency Model**:
- Prepare: 32 parallel workers (configurable in future release)
- Workload: 32 parallel workers (configurable per-operation)
- Cleanup: 32 parallel workers (matches prepare/workload)

**Pattern Resolution**:
- GET, DELETE, STAT: Pre-resolve patterns ‚Üí sample random URIs during execution
- PUT: Generate unique names dynamically (no pre-resolution needed)
- LIST: Operates on directories (no pre-resolution needed)

**Error Handling**:
- Prepare: Fails immediately on any PUT error (strict)
- Cleanup: Best-effort deletion (logs warnings, continues on errors)
- Workload: Propagates errors to maintain benchmark integrity

### üéØ Migration from v0.5.4

**Update your configs** to use glob patterns:

Old (broken):
```yaml
- op: delete
  path: "data/obj_{00000..19999}"
```

New (working):
```yaml
- op: delete
  path: "data/prepared-*.dat"
```

**No code changes needed** - just update YAML configs to use wildcard patterns.

---

## [0.5.4] - 2025-10-04

### üí• BREAKING CHANGES
- **Project Renamed**: `io-bench` ‚Üí `sai3-bench` (final name reflecting S3/Azure/I3 unified benchmarking)
- **Binary Names Changed**:
  - `io-bench` ‚Üí `sai3-bench`
  - `iobench-agent` ‚Üí `sai3bench-agent`
  - `iobench-ctl` ‚Üí `sai3bench-ctl`
  - `iobench-run` ‚Üí `sai3bench-run`
- **Module/Crate Name**: `io_bench` ‚Üí `sai3_bench` in Rust code

### üìù Documentation Updates
- Updated all documentation to reflect new project name
- Updated all command examples with new binary names
- Updated test configuration file paths (s3bench-test ‚Üí sai3bench-test)

### ‚ÑπÔ∏è Migration Notes
If upgrading from v0.5.3 (io-bench):
1. Update any scripts: `io-bench` ‚Üí `sai3-bench`
2. Update agent/controller scripts: `iobench-*` ‚Üí `sai3bench-*`
3. Rebuild: `cargo build --release`
4. All functionality remains identical‚Äîonly names changed

---

## [0.5.3] - 2025-10-04

### üéØ Realistic Size Distributions & Advanced Configurability
Surpasses MinIO Warp with realistic object size modeling and fine-grained concurrency control.

### ‚ú® New Features

#### Object Size Distributions (`src/size_generator.rs`)
**Problem**: Warp's "random" distribution is unrealistic. Real-world storage shows lognormal patterns (many small files, few large ones).

**Solution**: Three distribution types for PUT operations and prepare steps:

1. **Fixed Size** (backward compatible):
   ```yaml
   object_size: 1048576  # Exactly 1 MB
   ```

2. **Uniform Distribution** (evenly distributed):
   ```yaml
   size_distribution:
     type: uniform
     min: 1024        # 1 KB
     max: 10485760    # 10 MB
   ```

3. **Lognormal Distribution** (realistic - recommended):
   ```yaml
   size_distribution:
     type: lognormal
     mean: 1048576      # Mean: 1 MB
     std_dev: 524288    # Std dev: 512 KB
     min: 1024          # Floor
     max: 10485760      # Ceiling
   ```

**Implementation**:
- New `size_generator` module with `SizeSpec` enum and `SizeGenerator` struct
- Uses `rand_distr` crate for statistical distributions
- Rejection sampling for lognormal to respect min/max bounds
- Comprehensive unit tests for all distribution types

#### Per-Operation Concurrency
Fine-grained worker pool control per operation type:

```yaml
concurrency: 32  # Global default

workload:
  - op: get
    path: "data/*"
    weight: 70
    concurrency: 64  # Override: More GET workers
  
  - op: put
    path: "data/"
    object_size: 1048576
    weight: 30
    concurrency: 8   # Override: Fewer PUT workers
```

**Use cases**:
- Model read-heavy vs write-heavy workloads
- Simulate slow backend write performance
- Test different concurrency levels per operation type

**Implementation**:
- Per-operation semaphores (replaces single global semaphore)
- Optional `concurrency` field in `WeightedOp`
- Logs custom concurrency settings for visibility

#### Deduplication and Compression Control
Leverage `s3dlio`'s controlled data generation to test storage system efficiency:

```yaml
prepare:
  - path: "highly-dedupable/"
    num_objects: 100
    size_distribution:
      type: fixed
      size: 1048576
    dedup_factor: 10      # 10% unique blocks (90% duplicate)
    compress_factor: 1    # Uncompressible (random data)

workload:
  - op: put
    path: "compressible/"
    weight: 50
    size_distribution:
      type: lognormal
      mean: 1048576
      std_dev: 524288
    dedup_factor: 1       # 100% unique (no dedup)
    compress_factor: 3    # 67% zeros (3:1 compression ratio)
```

**Parameters**:
- `dedup_factor`: Controls block uniqueness
  - `1` = all unique blocks (no deduplication)
  - `2` = 1/2 unique blocks (50% dedup ratio)
  - `3` = 1/3 unique blocks (67% dedup ratio)
  - Higher values = more duplication
- `compress_factor`: Controls compressibility
  - `1` = random data (uncompressible)
  - `2` = 50% zeros (2:1 compression ratio)
  - `3` = 67% zeros (3:1 compression ratio)
  - Higher values = more compressible

**Use cases**:
- Test storage deduplication engines (NetApp, EMC, etc.)
- Validate compression effectiveness (ZFS, Btrfs)
- Measure real-world storage efficiency with realistic data patterns
- Benchmark cloud storage with various data types (logs, backups, media)

**Implementation**:
- Uses `s3dlio::generate_controlled_data(size, dedup, compress)` API
- Block-based generation (BLK_SIZE = 512 bytes) with Bresenham distribution
- Defaults both to `1` for backward compatibility
- Available in both `prepare` steps and `PUT` operations

### üìö Enhanced Documentation

#### Prepare Profiles
Documented realistic multi-tier preparation patterns in `docs/CONFIG.sample.yaml`:
- Small files (metadata, configs) with lognormal distribution
- Medium files (documents, images) with lognormal distribution
- Large files (videos, backups) with uniform distribution
- Complete production-clone example with 3 tiers

#### Advanced Remapping Examples
Added **N‚ÜîN (many-to-many)** remapping example to README:
```yaml
# Map 3 source buckets ‚Üí 2 destination buckets
remap:
  - pattern: "s3://source-1/(.+)"
    replacement: "s3://dest-a/$1"
  - pattern: "s3://source-2/(.+)"
    replacement: "s3://dest-a/$1"
  - pattern: "s3://source-3/(.+)"
    replacement: "s3://dest-b/$1"
```

#### Competitive Advantage Table
Added comprehensive comparison vs Warp in README showing superiority in:
- Size distributions (Uniform + Lognormal vs Random only)
- Concurrency control (Per-operation vs Global only)
- Backend support (5 backends vs S3 only)
- Output format (TSV vs Text)
- Memory usage (Constant vs High)

### üîß Improvements
- **Config backward compatibility**: Old `object_size` and `min_size`/`max_size` syntax still works
- **Migration helpers**: `get_size_spec()` method converts legacy syntax to new `SizeSpec`
- **Helper methods**: `SizeGenerator::description()` for logging, `expected_mean()` for validation
- **Human-readable formatting**: `human_bytes()` function for size display

### üß™ Testing
- **Unit tests**: 5 tests in `size_generator::tests` (fixed, uniform, lognormal, invalid specs, human_bytes)
- **Integration tests**:
  - `tests/configs/size_distributions_test.yaml` - Mixed lognormal, uniform, and fixed sizes
  - `tests/configs/per_op_concurrency_test.yaml` - Different concurrency per operation
- **Performance validation**: No regression (3649-5587 ops/s depending on config)

### üìä Test Results
```
size_distributions_test.yaml:
- Total ops: 18457 in 5.06s (3649 ops/s)
- Observed realistic size distributions across 9 buckets
- Lognormal PUT: Many small (1B-8KiB: 131 ops), few large (4MiB-32MiB: 3 ops)

per_op_concurrency_test.yaml:
- Total ops: 28268 in 5.06s (5587 ops/s)
- GET ops: 17074 (60% of total, concurrency=64)
- PUT ops: 11194 (40% of total, concurrency=4)
- Confirmed per-op concurrency via logs
```

### üèÜ Competitive Position
With v0.5.3, **sai3-bench surpasses MinIO Warp** in:
1. **Realistic workload modeling** - Lognormal distributions match real-world storage patterns
2. **Configurability** - Per-operation concurrency for advanced scenarios
3. **Backend support** - 5 protocols vs S3-only
4. **Output quality** - Machine-readable TSV with 13 columns
5. **Memory efficiency** - Constant memory streaming replay

## [0.5.2] - 2025-10-04

### üìö Documentation Cleanup & Polish
Streamlined documentation for clarity and removed obsolete files.

### üóëÔ∏è Removed Files
- **Old release notes**: Removed 4 version-specific release note files (v0.3.0, v0.3.2, v0.4.0, v0.4.3)
  - All release information consolidated into CHANGELOG.md
- **Completed design docs**: Removed 4 completed implementation documents
  - MIGRATION_PLAN.md (multi-backend migration complete)
  - OP_LOG_REPLAY_DESIGN.md (replay feature implemented)
  - REPLAY_FUTURE_WORK.md (features completed in v0.5.0)
  - DEVELOPMENT_NOTES.md (old v0.3.0 technical notes)

### ‚ú® Improvements
- **README.md**: Updated to v0.5.2 with comprehensive badges and modern examples
  - Added version, build, tests, license, and Rust version badges
  - Updated Quick Start with TSV export and advanced remapping examples
  - Added dedicated TSV Export section
  - Updated performance characteristics for all 5 backends
- **Documentation index**: Created docs/README.md for easy navigation
- **Warp parity tracking**: Added WARP_PARITY_STATUS.md showing 95% completion
- **Kept recent work**: Preserved v0.5.x implementation summaries and reference materials

### üìÇ Documentation Structure (40% reduction)
From 20 files to 12 essential documents:
- 4 user guides (USAGE, AZURE_SETUP, CONFIG samples)
- 4 reference docs (CHANGELOG, INTEGRATION_CONTEXT, Warp parity docs)
- 4 recent implementation records (v0.5.0/v0.5.1 summaries, POLARWARP analysis)

## [0.5.1] - 2025-10-04

### üéØ Machine-Readable Results & Enhanced Metrics
Phase 2.5 of Warp Parity: Add TSV export for automated analysis and complete size-bucketed histogram collection.

### ‚ú® New Features

#### TSV Export (`src/tsv_export.rs`)
- **Machine-readable results**: Export benchmark data in tab-separated format for automated analysis
- **CLI flag**: `--results-tsv <path>` for run command (creates `<path>-results.tsv`)
- **Format**: 13-column TSV with operation, size_bucket, bucket_idx, mean_us, p50_us, p90_us, p95_us, p99_us, max_us, avg_bytes, ops_per_sec, throughput_mibps, count
- **Per-bucket metrics**: Detailed statistics for each of 9 size buckets (zero, 1B-8KiB, 8KiB-64KiB, etc.)
- **Accurate throughput**: Uses actual bytes from SizeBins (not estimated), reported in MiB/s
- **Automated parsing**: Compatible with pandas, polars, and standard TSV tools

#### Enhanced Metrics Collection
- **Shared metrics module** (`src/metrics.rs`): Unified histogram infrastructure
- **Mean (average) latency**: Added alongside median (p50) for better statistical analysis
- **Size-bucketed histograms**: Now consistent across all execution modes (CLI commands and workload runs)
- **9 size buckets**: Tracks performance characteristics by object size from 0 bytes to >2GiB
- **Histogram exposure**: Summary struct now includes OpHists for TSV export
- **Actual byte tracking**: SizeBins.by_bucket provides real ops/bytes per size bucket

### üêõ Fixes
- **Workload histogram consistency**: Fixed missing size-bucketed collection in `workload::run()`
- **Code deduplication**: Removed 90+ lines of duplicate histogram code from main.rs

### üîß Changes
- **Default concurrency**: Increased from 20 to 32 workers
- **Cargo.toml**: Version bump to 0.5.1, added chrono dependency

### üìö Documentation
- **POLARWARP_ANALYSIS.md**: Reference analysis of polarWarp TSV format
- **V0.5.1_PLAN.md**: Complete implementation plan for TSV export
- **V0.5.1_PROGRESS.md**: Progress tracking and validation results

### ‚úÖ Validation
- **Multi-size test**: 4 size ranges (1KB, 128KB, 2MB, 16MB) showing 65x latency scaling
- **Mean vs median**: Demonstrated mean significantly higher than p50 for small objects (up to 934% difference)
- **TSV parsing**: Verified machine-readability with 13 properly formatted columns
- **Performance**: Maintained 19.6k ops/s on file backend with bucketed collection

## [0.5.0] - 2025-10-04

### üéØ Warp Parity Phase 2: Advanced Replay Remapping
Complete warp-replay compatibility with flexible URI remapping for multi-target testing and migration.

### ‚ú® New Features

#### Remap Engine (`src/remap.rs`)
Advanced URI remapping system supporting multiple mapping strategies:

1. **Simple 1‚Üí1 Remapping**
   - Direct bucket/prefix replacement
   - Use case: Migrate workloads between environments
   ```yaml
   rules:
     - match: {bucket: "prod"}
       map_to: {bucket: "staging", prefix: "test/"}
   ```

2. **1‚ÜíN Fanout**
   - Distribute operations across multiple targets
   - Three strategies: `round_robin`, `random`, `sticky_key`
   - Use case: Load testing, multi-region replication, chaos engineering
   ```yaml
   rules:
     - match: {bucket: "source"}
       map_to_many:
         targets:
           - {bucket: "dest1", prefix: ""}
           - {bucket: "dest2", prefix: ""}
           - {bucket: "dest3", prefix: ""}
         strategy: "round_robin"
   ```

3. **N‚Üí1 Consolidation**
   - Merge multiple sources to single target
   - Use case: Data consolidation, backup aggregation
   ```yaml
   rules:
     - match_any:
         - {bucket: "temp-1"}
         - {bucket: "temp-2"}
       map_to: {bucket: "consolidated", prefix: "merged/"}
   ```

4. **N‚ÜîN Regex-based Remapping**
   - Pattern matching with capture groups
   - Use case: Complex transformations, dynamic routing
   ```yaml
   rules:
     - regex: "^s3://prod-([^/]+)/(.*)$"
       replace: "s3://staging-$1/$2"
   ```

#### Fanout Strategies

**round_robin**: Sequential distribution for even load balancing
- Operation 1 ‚Üí Target A
- Operation 2 ‚Üí Target B
- Operation 3 ‚Üí Target C
- Operation 4 ‚Üí Target A (cycles)

**random**: Random selection for chaos testing
- Each operation randomly assigned to a target
- Useful for testing eventual consistency

**sticky_key**: Consistent hashing for session affinity
- Same object always goes to same target
- Deterministic based on object key hash
- Maintains data locality across runs

### üîß CLI Enhancements

#### New Replay Flag
```bash
# Basic remapping
sai3-bench replay --op-log production.tsv.zst --remap remap.yaml

# With speed control
sai3-bench replay --op-log ops.tsv.zst --remap fanout.yaml --speed 2.0

# Multi-target fanout
sai3-bench replay --op-log prod.tsv.zst --remap remap_fanout.yaml
```

### üèóÔ∏è Architecture

#### Core Components
- **`RemapConfig`**: YAML-driven configuration with ordered rules
- **`RemapRule`**: Enum supporting 4 rule types (Simple, Fanout, Consolidate, Regex)
- **`RemapEngine`**: Rule execution engine with state management
- **`ParsedUri`**: S3/file URI parser extracting scheme/bucket/prefix/key

#### Integration
- Extended `ReplayConfig` with optional `remap_config` field
- Remap applied before each operation execution in replay loop
- Zero overhead when remapping not used

### üìù Configuration Schema

#### MatchSpec (for matching source URIs)
```yaml
match:
  host: "optional-host"      # Match specific host (rarely used)
  bucket: "bucket-name"      # Match specific bucket
  prefix: "path/prefix/"     # Match objects under prefix
```

#### TargetSpec (for destination URIs)
```yaml
map_to:
  bucket: "dest-bucket"
  prefix: "dest/path/"       # Empty = preserve original path
```

#### Path Preservation Logic
- **Empty target prefix**: Preserve original path structure
  - `s3://src/data/file.dat` ‚Üí `s3://dest/data/file.dat`
- **Non-empty target prefix**: Replace original prefix
  - `s3://src/old/file.dat` + `new/` ‚Üí `s3://dest/new/file.dat`

### ‚úÖ Testing & Validation

#### Unit Tests (10 tests, all passing)
- **`test_parse_uri_s3`**: S3 URI parsing (`s3://bucket/prefix/key`)
- **`test_parse_uri_file`**: File URI parsing (`file:///path/to/file`)
- **`test_simple_remap`**: 1‚Üí1 bucket/prefix replacement
- **`test_fanout_round_robin`**: Sequential distribution across 3 targets
- **`test_regex_remap`**: Pattern-based prod‚Üístaging transformation
- **`test_no_match_returns_original`**: Fallback to original URI
- Existing replay tests continue to pass

#### Build Verification
```bash
cargo test --lib
test result: ok. 10 passed; 0 failed

cargo build --release
Finished `release` profile [optimized] in 13.20s
```

### üî® Implementation Details

#### State Management
- **Round-robin**: `HashMap<rule_idx, counter>` per rule
- **Sticky-key**: `HashMap<key, target_idx>` for consistent hashing
- Thread-safe with `Arc<Mutex<>>`
- Per-rule state prevents interference

#### Ordered Rule Matching
- Rules evaluated sequentially; first match wins
- Allows specific rules before general rules
- Override patterns via ordering

#### URI Parsing
```rust
// S3 URI structure
s3://bucket/prefix/path/to/key.dat
  ‚îú‚îÄ‚îÄ scheme: "s3"
  ‚îú‚îÄ‚îÄ bucket: "bucket"
  ‚îú‚îÄ‚îÄ prefix: "prefix/path/to/"
  ‚îî‚îÄ‚îÄ key: "key.dat"
```

### üì¶ Files Modified

**New Files**:
- `src/remap.rs` (488 lines): Complete remap engine
- `tests/configs/remap_examples.yaml`: Comprehensive configuration examples
- `tests/configs/remap_fanout_test.yaml`: Simple fanout test

**Modified Files**:
- `Cargo.toml`: Version 0.4.3 ‚Üí 0.5.0
- `src/lib.rs`: Added `pub mod remap;`
- `src/replay_streaming.rs`: Extended `ReplayConfig`, integrated remap engine
- `src/main.rs`: Added `--remap` CLI flag, config loading
- `.github/copilot-instructions.md`: Updated to v0.5.0-dev

### üéì Example Configurations

#### Complete Multi-Rule Configuration
```yaml
rules:
  # Rule 1: Simple bucket rename
  - match: {bucket: "old-bucket", prefix: "logs/"}
    map_to: {bucket: "new-bucket", prefix: "archived/"}
  
  # Rule 2: Fanout for load testing
  - match: {bucket: "source"}
    map_to_many:
      targets:
        - {bucket: "replica1", prefix: ""}
        - {bucket: "replica2", prefix: ""}
        - {bucket: "replica3", prefix: ""}
      strategy: "round_robin"
  
  # Rule 3: Consolidation
  - match_any:
      - {bucket: "temp-1"}
      - {bucket: "temp-2"}
    map_to: {bucket: "consolidated", prefix: "merged/"}
  
  # Rule 4: Regex transformation
  - regex: "^s3://prod-([^/]+)/(.*)$"
    replace: "s3://staging-$1/$2"
```

### üêõ Bug Fixes

#### Path Structure Preservation
**Fixed**: URI remapping now correctly handles:
- Empty target prefix ‚Üí preserve original path structure
- Non-empty target prefix ‚Üí replace prefix, keep key
- Prevents both "lost path" and "double path" issues

#### S3 URI Parsing
**Fixed**: S3 URIs correctly parsed without "host" component
- `s3://bucket/path` ‚Üí bucket="bucket", not host="bucket"
- Consistent with S3 URL structure

### üöÄ Roadmap

#### v0.5.1 (Phase 3): UX Polish
- Enhanced documentation with examples
- Variable substitution in configs
- Improved CLI help text
- Warp-compatible defaults

#### v0.5.2 (Phase 4): Testing & Validation
- Integration tests with real backends
- Performance benchmarking
- Warp comparison validation
- Production-ready hardening

---

## [0.4.3] - 2025-10-04

### üéØ Warp Parity Phase 1: Prepare/Pre-population
MinIO Warp compatibility features enabling near-identical test workflows between sai3-bench and Warp.

### ‚ú® New Features

#### Prepare Step (Pre-population)
- **ensure_objects**: Guarantee test objects exist before timed workload execution
  - Configurable count, size range (min_size/max_size), and fill pattern (zero/random)
  - Skips creation if sufficient objects already exist (idempotent)
  - Progress bar with ops/sec tracking during preparation
  - Example: Create 50K objects @ 1 MiB each before 5-minute mixed workload
  
- **cleanup**: Optional automatic cleanup after test completion
  - Only deletes objects created during prepare step
  - Preserves objects created during actual workload
  - CLI override via `--no-cleanup` flag

#### CLI Enhancements
- `--prepare-only`: Execute prepare step then exit (for pre-populating test data)
- `--no-cleanup`: Override config cleanup setting (keep all objects)
- Examples:
  ```bash
  # Prepare objects once, run tests multiple times
  sai3-bench run --config mixed.yaml --prepare-only
  sai3-bench run --config mixed.yaml
  sai3-bench run --config mixed.yaml
  
  # Run with cleanup disabled
  sai3-bench run --config mixed.yaml --no-cleanup
  ```

### üîß Configuration Schema Extensions

#### New PrepareConfig Structure
```yaml
prepare:
  ensure_objects:
    - base_uri: "s3://bucket/prefix/"
      count: 50000
      min_size: 1048576  # 1 MiB (default)
      max_size: 1048576  # 1 MiB (default)
      fill: zero         # "zero" or "random" (default: zero)
  cleanup: false         # Auto-cleanup after test (default: false)
```

### üéØ Warp Compatibility Examples
```yaml
# Warp-style mixed workload
duration: 300s      # 5 minutes
concurrency: 20     # Match Warp default (changed from 16)

prepare:
  ensure_objects:
    - base_uri: "s3://bench/test/"
      count: 50000
      min_size: 1048576
  cleanup: false

workload:
  - {weight: 50, op: get, path: "test/*"}
  - {weight: 30, op: put, path: "test/*", object_size: 1048576}
  - {weight: 10, op: list, path: "test/"}
```

### ÔøΩ Bug Fixes & Improvements

#### Progress Bar Consistency
- **Unified Visual Style**: All progress bars now use consistent block characters (`‚ñà‚ñà‚ñà‚ñà`)
  - Prepare phase: `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 1000/1000 objects`
  - Test phase: `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 10/10s`
  - Cleanup phase: `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100/100 objects`
  - Previously: Test phase used ugly `###>---` style

#### s3dlio Native Methods
- **STAT Operations**: Now use s3dlio's native `stat()` method (v0.8.8+)
  - Previously: Used workaround with `get()` to measure size
  - Now: Proper HEAD-like metadata operations
  - Benefit: More efficient, less data transfer, correct semantics
  
#### Pattern Matching Clarification
- **Documented Approach**: Added comments explaining sai3-bench's URI pattern matching
  - s3dlio's ObjectStore doesn't provide native glob support in `list()`
  - sai3-bench implements list-and-filter for `*` patterns at application level
  - This is correct: object stores don't have native glob (S3, Azure, GCS all work this way)
  - For local file operations, s3dlio has `generic_upload_files()` with glob crate

### ÔøΩüî® Implementation Details
- **src/config.rs**: Added `PrepareConfig`, `EnsureSpec`, `FillPattern` types
  - `default_concurrency()` changed from 16 ‚Üí 20 (match Warp)
  - All new fields are optional (backward compatible)
  
- **src/workload.rs**: New prepare infrastructure + improvements
  - `prepare_objects()`: Create/verify test objects with progress tracking
  - `cleanup_prepared_objects()`: Delete only created objects
  - `PreparedObject`: Tracks URI, size, and created flag
  - `stat_object_multi_backend()`: Updated to use s3dlio's `stat()` method
  - Progress bar style unified across all phases
  - Uses s3dlio ObjectStore for all operations (consistent with existing code)

- **src/main.rs**: Updated `run_workload()` command handler
  - Three-phase execution: Prepare ‚Üí Test ‚Üí Cleanup
  - CLI flag processing for `--prepare-only` and `--no-cleanup`
  - Conditional phase execution with clear console output

### ‚úÖ Validation & Testing
- **Performance**: 500+ objects/sec prepare speed (1 MiB objects on local file backend)
- **Idempotency**: Repeated prepare-only calls skip existing objects
- **Cleanup**: Verified only prepared-*.dat removed, test-* preserved
- **CLI Flags**: All combinations tested (prepare-only, no-cleanup, default)
- **Progress Bars**: Visual consistency verified across all phases
- **STAT Operations**: Confirmed using native s3dlio method
- **Example Configs**: `tests/configs/warp_parity_mixed.yaml`, `tests/configs/warp_cleanup_test.yaml`

### üì¶ Dependencies
- **rand**: Updated API usage (deprecated `thread_rng()` ‚Üí `rng()`, `gen_range()` ‚Üí `random_range()`)
- **indicatif**: Progress bars for prepare and cleanup phases
- **s3dlio v0.8.8+**: Leveraging native `stat()` method

### üöÄ Roadmap
- **v0.5.0**: Phase 2 - Advanced replay remapping (1‚ÜíN, N‚Üí1, N‚ÜîN)
- **v0.5.1**: Phase 3 - UX polish and documentation
- **v0.5.2**: Phase 4 - Comprehensive testing and Warp comparison validation

---

## [0.4.2] - 2025-10-03

### üåê Google Cloud Storage Backend Support
- **New Backend**: Added comprehensive GCS support as the 5th storage backend
  - **URI Schemes**: Both `gs://bucket/prefix/` and `gcs://bucket/prefix/` supported
  - **Authentication**: Google Application Default Credentials (ADC) integration
    - Service account JSON via `GOOGLE_APPLICATION_CREDENTIALS`
    - GCE/GKE metadata server (automatic on Google Cloud)
    - gcloud CLI credentials
  - **Full Operation Support**: GET, PUT, DELETE, LIST, STAT operations
  - **Performance**: 9-11 MB/s for large objects (5MB), ~400-600ms latency for small objects

### üîß Implementation Details
- **src/workload.rs**: Added `Gcs` variant to `BackendType` enum
  - URI scheme detection for `gs://` and `gcs://`
  - Backend name: "Google Cloud Storage"
  - URI path building compatible with S3-style paths
- **src/main.rs**: Updated URI validation to accept GCS schemes
  - Added `gs` and `gcs` to supported schemes list
  - Updated error messages to include `gs://`

### ‚úÖ Testing & Validation
- **8 Comprehensive Integration Tests** (tests/gcs_tests.rs)
  - Backend detection and ObjectStore creation
  - PUT/GET/DELETE cycle with content verification
  - LIST operations (5 objects)
  - STAT metadata retrieval
  - Concurrent operations (10 objects with tokio::spawn)
  - Large object handling (5MB at 9.48/11.29 MB/s)
  - Alternate URI scheme testing (gs:// and gcs://)
  - **All tests passed** against real GCS bucket (signal65-russ-b1)
  
- **Shell Test Suite** (tests/gcs_backend_test.sh)
  - 9 comprehensive tests covering health check, operations, concurrency
  - Professional output with colors and progress tracking
  - Op-log generation and workload testing

- **YAML Configuration** (tests/configs/gcs_test.yaml)
  - Mixed workload template (PUT 30%, GET 50%, LIST 10%, STAT 10%)
  - Environment variable support for `GCS_BUCKET`

### üì¶ Dependencies
- **tokio**: Added to dev-dependencies with "full" features for async tests

### üìù Documentation
- **README.md**: Updated from "4 storage backends" to "5 storage backends"
- **GCS_INTEGRATION_COMPLETE.md**: Comprehensive integration documentation
  - Authentication setup instructions
  - Usage examples and performance characteristics
  - Test results and verification checklist

### üéØ Real-World Testing
Successfully tested against Google Cloud Storage:
- Bucket: gs://signal65-russ-b1/
- Project: signal65-testing
- All 8 Rust integration tests passed in 14.76s
- CLI operations verified (health, put, get, delete, list)

## [0.4.1] - 2025-10-03

### üöÄ Major Updates
- **Streaming Op-log Replay**: Memory-efficient replay using s3dlio-oplog streaming reader
  - **Constant Memory Usage**: ~1.5 MB regardless of op-log size (vs. unbounded Vec-based approach)
  - **Background Decompression**: Parallel zstd decompression in dedicated thread
  - **Tunable Performance**: Environment variables for buffer size and chunk size
    - `S3DLIO_OPLOG_READ_BUF`: Channel buffer size (default: 1024 entries)
    - `S3DLIO_OPLOG_CHUNK_SIZE`: Decompression chunk size (default: 1 MB)
  - **Shared Types**: Uses `s3dlio_oplog::{OpLogEntry, OpType, OpLogStreamReader}` for consistency

### üîß Technical Improvements
- **Non-logging Replay Operations**: Added `*_no_log()` variants in workload.rs
  - Prevents circular logging during replay (replay operations are not logged)
  - Eliminates "sending on a closed channel" errors when global logger is finalized
  - Functions: `get_object_no_log()`, `put_object_no_log()`, `list_objects_no_log()`, `stat_object_no_log()`, `delete_object_no_log()`

- **Deprecated Legacy Replay**: Old `src/replay.rs` marked deprecated with warnings
  - Backup preserved in `src/replay_v040_backup.rs`
  - New streaming implementation in `src/replay_streaming.rs`
  - Updated `src/main.rs` to use streaming replay by default

### üì¶ Dependencies
- **s3dlio**: Updated from tag v0.8.12 to branch "main" (v0.8.19+)
  - Includes 10+ releases with bug fixes and performance improvements
  - GCS backend support (gs:// and gcs:// URIs) - ready for future integration
- **s3dlio-oplog**: New dependency for streaming op-log parsing
  - Separate workspace member in s3dlio repository
  - Provides `OpLogStreamReader` for memory-efficient iteration

### ‚úÖ Testing
- **Comprehensive Integration Tests**: 6 tests validating streaming replay
  - Round-trip test: s3dlio generates op-log ‚Üí sai3-bench replays
  - Memory efficiency test: 100+ operations with constant memory
  - URI remapping test: Replay to different storage backend
  - Error handling test: Continue-on-error functionality
  - Concurrent limits test: Configurable concurrency controls
  - Streaming reader test: Iterator-based processing
- **Global Logger Workaround**: Tests structured to work with s3dlio's singleton logger
  - One generation test creates op-log (calls finalize once)
  - Other tests read and replay existing op-logs (no logging)

### üìù Documentation
- **docs/S3DLIO_UPDATE_PLAN.md**: Comprehensive update strategy for s3dlio v0.8.19+
- **STREAMING_REPLAY_COMPLETE.md**: Implementation summary and validation results

### üîÆ Future Work
- GCS backend integration (Phase 2 - ready in s3dlio)
- Advanced URI remapping (M:N, sticky sessions)
- Op-log filtering and transformation

## [0.4.0] - 2025-10-01
### Added
- **Op-log Replay**: Full timing-faithful workload replay from TSV op-log files
  - Absolute timeline scheduling for microsecond-precision timing (~10¬µs accuracy)
  - Auto-detection of zstd compression (.zst extension)
  - Speed multiplier (`--speed`) for faster/slower replay
  - Target URI remapping (`--target`) for simple 1:1 backend retargeting
  - Support for all 5 operation types: GET, PUT, DELETE, LIST, STAT
  - Error handling with `--continue-on-error` flag
  - Uses `s3dlio::data_gen::generate_controlled_data()` for PUT operations
  - Microsecond-precision sleep via `std::thread::sleep` wrapped in `tokio::task::spawn_blocking`

### Dependencies
- Added `csv = "1.3"` for TSV parsing
- Added `chrono = "0.4"` for timestamp handling
- Added `zstd = "0.13"` for op-log decompression

## [0.3.2] - 2025-09-30

### ‚ú® NEW FEATURES
- **Universal Operation Logging (Op-Log)**: Comprehensive operation tracing across all storage backends
  - **Multi-Backend Support**: Captures operations from file://, direct://, s3://, and az:// backends
  - **Automatic Compression**: All op-logs are zstd-compressed (.tsv.zst format)
  - **Detailed Metrics**: Records timestamps, durations, sizes, errors for every operation
  - **CLI Integration**: New `--op-log <PATH>` global flag for all commands
  - **Replay Ready**: TSV format designed for future workload replay functionality (planned v0.4.0)

- **Enhanced Logging System**: Unified tracing framework with pass-through to s3dlio
  - **Verbosity Levels**: 
    - No flags: Warnings and errors only
    - `-v`: INFO level for sai3-bench, minimal s3dlio output
    - `-vv`: DEBUG level for sai3-bench, INFO level for s3dlio (operational details)
    - `-vvv`: TRACE level for sai3-bench, DEBUG level for s3dlio (full debugging)
  - **Cascading Levels**: sai3-bench verbosity automatically configures s3dlio logging (one level less)
  - **Unified Framework**: Both crates use `tracing` crate for consistent log formatting

### üöÄ DEPENDENCY UPDATES
- **s3dlio**: Upgraded to v0.8.12 (from git tag, no local patches)
  - Universal op-log support across all backends
  - Migration from `log` to `tracing` crate
  - Removed aws-smithy-http-client patch (no longer needed)
  - Operation logger API: `init_op_logger()`, `finalize_op_logger()`, `global_logger()`

### üìä OPERATION LOGGING FORMAT
```tsv
idx  thread  op  client_id  n_objects  bytes  endpoint  file  error  start  first_byte  end  duration_ns
```
- Compressed with zstd (typically 10-20x reduction)
- Compatible with standard TSV tools after decompression
- Design documented in `docs/OP_LOG_REPLAY_DESIGN.md` for future replay feature

### üîß TECHNICAL IMPROVEMENTS
- **Build System**: Simplified dependency management, removed local patches
- **Logging Architecture**: EnvFilter configuration for per-crate log levels
- **ObjectStore Integration**: Enhanced with logger support via `store_for_uri_with_logger()`
- **All Operations Instrumented**: GET, PUT, DELETE, LIST, STAT operations support op-logging

### üìñ DOCUMENTATION
- Added `docs/OP_LOG_REPLAY_DESIGN.md`: Complete replay feature specification for v0.4.0
- Updated `.github/copilot-instructions.md`: Added ripgrep (rg) usage guide and op-log examples
- Enhanced CLI help text: Clarified compression behavior and use cases

### üß™ TESTING
- Validated op-log capture across all backends (file://, s3://, az://)
- Verified zstd compression and decompression workflow
- Tested logging level pass-through with -v, -vv, -vvv flags
- Confirmed 59K+ operations captured from 5-second workload (9MB compressed)

## [0.3.1] - 2025-09-30

### ‚ú® NEW FEATURES
- **Interactive Progress Bars**: Professional progress visualization for all operations
  - **Time-based Progress**: Smooth animated progress bars for timed workloads with elapsed/remaining time
  - **Operation-based Progress**: Object count progress tracking for GET, PUT, DELETE commands
  - **Smart Contextual Messages**: Dynamic progress messages showing concurrency, data sizes, and completion status
  - **ETA Calculations**: Estimated time remaining for all operations
  - **Async-friendly Design**: Non-blocking progress updates every 100ms

### üöÄ USER EXPERIENCE IMPROVEMENTS
- **Enhanced Default Output**: Improved feedback without verbose flags
  - **Preparation Status**: Clear indication of workload setup and GET pattern resolution
  - **Execution Progress**: Real-time visual feedback during operation execution
  - **Completion Summary**: Informative completion messages with performance data
- **Better Visual Feedback**: Colored progress bars with professional styling
- **Informative Progress Messages**: Context-aware messages showing worker counts and data transfer amounts

### üîß TECHNICAL ENHANCEMENTS
- **indicatif Integration**: Added professional progress bar library for smooth animations
- **Concurrent Progress Tracking**: Thread-safe progress updates using `Arc<ProgressBar>`
- **Minimal Performance Impact**: Efficient 100ms update intervals for smooth user experience
- **Cross-terminal Compatibility**: Progress bars work across different terminal widths and configurations

### üì¶ DEPENDENCY UPDATES
- Added `indicatif = "0.17"` for progress bar functionality

## [0.3.0] - 2025-09-30

### üéâ MAJOR RELEASE: Complete Multi-Backend & Naming Transformation

This release represents a fundamental transformation from an S3-only tool to a comprehensive multi-protocol I/O benchmarking suite.

### üí• BREAKING CHANGES
- **Project Renamed**: `s3-bench` ‚Üí `sai3-bench` (reflects multi-protocol nature)
- **Binary Names Changed**:
  - `s3-bench` ‚Üí `sai3-bench`
  - `s3bench-agent` ‚Üí `sai3bench-agent`
  - `s3bench-ctl` ‚Üí `sai3bench-ctl`
  - `s3bench-run` ‚Üí `sai3bench-run`
- **gRPC Protocol**: Package renamed from `s3bench` to `iobench`

### ‚ú® NEW FEATURES
- **Complete Multi-Backend Support**: Full CLI and workload support for all 4 storage backends
  - File system (`file://`) - Local filesystem operations
  - Direct I/O (`direct://`) - High-performance direct I/O
  - S3 (`s3://`) - Amazon S3 and S3-compatible storage
  - Azure Blob (`az://`) - Microsoft Azure Blob Storage
- **Unified CLI Interface**: All commands work consistently across all backends
- **Enhanced Performance Metrics**: Microsecond precision latency measurements
- **Azure Blob Storage**: Full support with proper authentication and URI format
- **Advanced Glob Patterns**: Cross-backend wildcard support for GET operations

### üöÄ MAJOR IMPROVEMENTS
- **Phase 1: CLI Migration** - Complete transition from S3-specific to multi-backend commands
- **Phase 2: Dependency Analysis** - Thorough investigation and documentation of s3dlio requirements
- **Phase 3: Backend Validation** - Systematic testing and validation of all storage backends
- **Microsecond Precision**: Enhanced HDR histogram metrics with microsecond-level accuracy
- **URI Validation**: Comprehensive URI format validation across all backends
- **Error Handling**: Improved error messages and backend-specific guidance

### üîß TECHNICAL ENHANCEMENTS
- **ObjectStore Abstraction**: Complete migration to s3dlio ObjectStore trait
- **Glob Pattern Matching**: Fixed URI scheme normalization for pattern matching
- **Azure Authentication**: Proper support for Azure storage account keys and CLI authentication
- **Configuration System**: Enhanced YAML configuration with `target` URI support
- **Distributed gRPC**: Validated and tested distributed agent/controller functionality

### üêõ BUG FIXES
- **Direct I/O Glob Patterns**: Fixed glob pattern matching for direct:// backend operations
- **Azure URI Format**: Corrected URI format to `az://STORAGE_ACCOUNT/CONTAINER/`
- **URI Scheme Normalization**: Resolved cross-scheme pattern matching issues
- **Build Dependencies**: Documented and resolved aws-smithy-http-client patch requirements

### üìö DOCUMENTATION
- **Azure Setup Guide**: Comprehensive Azure Blob Storage configuration documentation
- **Multi-Backend Examples**: Updated all examples to showcase 4-backend support
- **Configuration Samples**: Enhanced config examples with environment variable usage
- **Phase Implementation Reports**: Detailed documentation of migration phases
- **Backend-Specific Guides**: Tailored setup instructions for each storage backend

### üß™ TESTING & VALIDATION
- **Backend Test Suite**: Created comprehensive test configurations for all backends
- **Performance Validation**: Verified performance characteristics across all storage types
- **Integration Tests**: Updated gRPC integration tests with new binary names
- **Azure Connectivity**: Validated real-world Azure Blob Storage operations

### üìä PERFORMANCE CHARACTERISTICS
- **File Backend**: 25k+ ops/s, sub-millisecond latencies
- **Direct I/O Backend**: 10+ MB/s throughput, ~100ms latencies
- **Azure Blob Storage**: 2-3 ops/s, ~700ms latencies (network dependent)
- **Cross-Backend Workloads**: Tested mixed workload scenarios

### üîß INTERNAL CHANGES
- **Protobuf Schema**: Renamed from `s3bench.proto` to `iobench.proto`
- **Module Structure**: Updated all internal references and imports
- **Binary Generation**: Updated build system for new binary names
- **Test Framework**: Adapted integration tests for renamed binaries

### üìã MIGRATION GUIDE
For users upgrading from 0.2.x:
1. Update binary names in scripts and automation
2. Review Azure URI format if using Azure backend
3. Update any gRPC integrations to use `iobench` package
4. Verify environment variables for Azure authentication

### üéØ NEXT STEPS
- Complete S3 backend validation when access becomes available
- Enhanced distributed testing capabilities
- Performance optimization across backends
- Additional storage backend support

---

## [0.2.3] - Previous Release

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.2.3] - 2025-09-30

### Added
- **Microsecond Precision Metrics**: All timing measurements now use microsecond precision instead of milliseconds
- **Three-Category Operation Tracking**: Added META-DATA operations category alongside GET and PUT
  - LIST operations: Directory/prefix listings with timing metrics
  - STAT operations: Object metadata queries (HEAD requests)
  - DELETE operations: Object removal with timing tracking
- Enhanced configuration support for new operation types: `list`, `stat`, `delete`
- Comprehensive per-operation reporting with separate latency percentiles for GET, PUT, and META-DATA
- Updated reporting displays with microsecond (¬µs) units across all binaries

### Changed
- **BREAKING**: Changed timing precision from milliseconds to microseconds in all metrics
- **BREAKING**: Updated histogram bounds from (1, 60_000, 3) to (1, 60_000_000, 3) for microsecond scale
- **BREAKING**: Renamed struct fields from `p50_ms/p95_ms/p99_ms` to `p50_us/p95_us/p99_us`
- Enhanced `OpSpec` enum with `List`, `Stat`, and `Delete` variants
- Updated `Summary` and `OpAgg` structures to include `meta` fields for metadata operations
- Improved help message to reflect multi-backend I/O testing capabilities

### Fixed
- Consistent microsecond reporting across main CLI and run binary
- Proper operation category separation in metrics collection and reporting
- Enhanced ObjectStore integration for metadata operations

## [0.2.2] - 2025-09-30

### Added
- **Stage 2 Migration Complete**: Full ObjectStore trait implementation for all operations
- Multi-backend URI support: `file://`, `direct://`, and `s3://` schemes
- Comprehensive logging infrastructure with tracing crate
- CLI verbosity options: `-v` for info level, `-vv` for debug level logging
- Debug and info logging throughout workload execution and ObjectStore operations
- File backend testing and validation with successful operations

### Changed
- **BREAKING**: Migrated from AWS SDK direct calls to ObjectStore trait for all operations
- Replaced `get_object()` and `put_object_async()` with `get_object_multi_backend()` and `put_object_multi_backend()`
- Updated URI handling to use full URIs with ObjectStore instead of bucket/key splitting
- Improved prefetch operations to use `ObjectStore::list()` instead of AWS SDK `list_objects_v2()`
- Enhanced configuration pattern matching to use proper destructuring

### Removed
- Deprecated `prefetch_keys()` and `list_keys_async()` functions using AWS SDK
- Unused AWS SDK imports: `aws_config`, `aws_sdk_s3`, `RegionProviderChain`
- Legacy `parse_s3_uri` usage in workload.rs (still available in main.rs for CLI operations)
- Dead code: unused struct fields and variables in pattern matching

### Fixed
- All compiler warnings resolved through proper code analysis (not cheap underscore fixes)
- ObjectStore URI usage corrected to use full URIs following s3dlio test patterns
- Redundant pattern destructuring where config methods handled field extraction
- Proper handling of GetSource struct with only necessary fields

### Performance
- File backend testing shows excellent performance: 25,462 ops/s with 38.77 MB/s throughput
- Multi-backend operations maintain low latency: p50: 1ms, p95: 1ms, p99: 1ms
- Successful concurrent operations with proper semaphore-based concurrency control

### Technical Details
- ObjectStore operations now use `store_for_uri()` for automatic backend detection
- All operations handle full URIs natively without bucket/key splitting
- Logging provides visibility into ObjectStore creation and operation execution
- Clean compilation with zero warnings after proper code analysis

## [0.2.1] - 2025-09-29

### Added
- s3dlio v0.8.7 integration with ObjectStore trait support
- Multi-backend foundation for file:// and direct:// URI support
- Fork patch system for aws-smithy-http-client v1.1.1 compatibility

### Changed
- **BREAKING**: Updated to s3dlio v0.8.7 (pinned to rev cd4ee2e)
- Updated import structure to support both legacy s3_utils and new object_store APIs
- Improved BackendType::from_uri to use string matching for URI scheme detection

### Fixed
- Compilation issues with AWS SDK version conflicts
- list_objects function calls now include required recursive parameter
- Removed unused imports, variables, and dead code warnings
- Applied aws-smithy-http-client fork patch to expose hyper_builder method

### Technical Details
- All binaries (s3-bench, s3bench-agent, s3bench-ctl) compile successfully
- Maintains backward compatibility with existing S3 workloads
- Prepares foundation for Stage 2 migration to ObjectStore trait operations

## [0.2.0] - Previous Release
- Initial distributed execution with gRPC agents
- HDR histogram metrics collection
- Single-node CLI and multi-agent controller modes